%\documentclass[aps,prd,twocolumn,floatfix]{revtex4}   % style for Physical Review B and AJP are similar
\documentclass[twocolumn,floatfix,nofootinbib]{revtex4}   % style for Physical Review B and AJP are similar
%\documentclass[12pt,aps,prb,preprint]{revtex4}

\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsthm}

\theoremstyle{theorem}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{sublem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{ax}[thm]{Axiom}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{clm}[thm]{Claim}
\newtheorem{lemdef}[thm]{Lemma-Definition}


\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{ques}[thm]{Question}
\newtheorem{cons}[thm]{\rm\bfseries{Construction}}
\newtheorem{exm}[thm]{Example}
\newtheorem{conds}[thm]{Condition}
\newtheorem{proper}[thm]{Property}
\newtheorem{defnlem}[thm]{Definition-Lemma}
\newtheorem*{assump1}{Classical assumption}
\newtheorem*{assump2}{Determinism and Reversibility assumption}
\newtheorem{warn}[thm]{Warning}
\newtheorem{situ}[thm]{Situation}

\begin{document}

\title{From physical principles to Hamiltonian mechanics}
\author{Gabriele Carcassi}
\affiliation{Brookhaven National Laboratory, Upton, NY 11973}
\email{carcassi@bnl.gov}
\date{May 23, 2014}

\begin{abstract}
We derive the Hamiltonian formulation of classical mechanics directly, without reference to Lagrangian mechanics.
We start from the definition of states in terms of labels used to identify them, and show how, under a deterministic and
reversible process, the conservation of the cardinality of the labels leads to Hamilton's equations.\end{abstract}

\maketitle

\section{Introduction}
In physics Hamiltonian mechanics is usually presented as a reformulation of Lagrangian mechanics, which is how it was originally developed. In this work we aim to derive it on its own, starting from simple physical assumptions, without any reference to Lagrangian mechanics.

We'll give simple definitions of states in terms of labels and of determinism/reversibility in terms of bijective maps; define a metric based on the idea that bijective maps preserve the cardinality of labels; obtain Hamilton's equations from the invariance of that metric.

No mathematical breakthrough should be expected: the goal, after all, is to derive the \emph{known} framework from a set of \emph{simple} definitions in the most \emph{obvious} way possible. No proof is longer than a couple of paragraphs, so the word \emph{theorem} is avoided in favor of \emph{proposition} and \emph{corollary}.

\section{States}
\textbf{State definition}. We assume we have a physical system that can be found in different configurations. We call each unique configuration \emph{configuration state} and assign it a \emph{label}. For our purposes, it does not matter what the label is (a measurement, a way to prepare the system, future behavior, ...), just that it exists.

\begin{defn}\label{statedef}
Let $I$ be a set of labels. Let $\mathbbm{C}_{I}$ be the set of all possible configuration states of our system identified by the labels. $\forall i \in I, \exists \mathbbm{c}_{i} \in \mathbbm{C}_{I}$.
\end{defn}

The following definitions allow us to use multiple labels.

\begin{defn}\label{labelsCombine}
Let $I_1$ and $I_2$ be two sets of labels. We call $I = \langle I_1, I_2 \rangle$ a \emph{combined set of labels} if an injective map $f:I \rightarrowtail I_1, I_2$ exists that maps an element of $I$ to a pair of elements of $I_1$ and $I_2$.
\end{defn}

\begin{defn}\label{labelsOrth}
The two label sets $I_1$ and $I_2$ are said to be \emph{orthogonal} iff $I = \langle I_1, I_2 \rangle = I_1 \times I_2$.
\end{defn}

The following assumptions and definitions allow us to describe a configuration state by its parts.

\begin{assump1}\label{classical}
The system is infinitely reducible: it can be thought as composed by two or more similar but smaller systems, each in its own configuration state, which also can be thought as composed by two or more, ad infinitum.
\end{assump1}

\begin{defn}\label{classicalPhaseSpace}
Let $\mathbbm{S}_I$ be the set of all possible configuration states of the infinitesimal subdivision. We call this set \emph{phase space}. We call each element $\mathbbm{s}_i$ a \emph{state}.
\end{defn}

\begin{cor}\label{classicalDistribution}
Each classical configuration state $\mathbbm{c}_j \in \mathbbm{C}_J$ can be identified by a distribution over states: $\mathbbm{c}_j=\sum D(i) \mathbbm{s}_i$, where $D:I\rightarrow\mathbb{R}$ measures how much of the system can be found in each $\mathbbm{s}_i$. In physics term, the distribution can be visualized as a histogram over the discrete labels.
\end{cor}

Under the classical assumption, we can then limit ourselves to study the labels of states and their properties without loss of generality.

\textbf{State mapping}. We will now concentrate on deterministic and reversible evolution.

\begin{assump2}
The system undergoes deterministic (future state identified by the present state) and reversible (past state identified by the present state) evolution.
\end{assump2}

\begin{prop}\label{detrevMap}
Let $I$ be a set of labels of a system that undergoes deterministic and reversible evolution. There exists a bijective map $f:I \leftrightarrow I$.
\end{prop}

\begin{cor}\label{detrevDist}
The evolution of a classical configuration state $\mathbbm{c}_j=\sum D(i) \mathbbm{s}_i$ under a bijective map is given by $\mathbbm{c}_{j'}=\sum D'(i) \mathbbm{s}_{i}=\sum D(f^{-1}(i)) \mathbbm{s}_{i}$.
\end{cor}

Mathematically, assuming determinism and reversibility means studying bijective maps. The evolution of a distribution simply moves the elements around: the bars of the histogram move place, but keep the same height.

\begin{defn}\label{labelsIndep}
Let $I_1$ and $I_2$ be two orthogonal sets of labels and $I = I_1 \times I_2$. Let $f:I \leftrightarrow I$ be a bijective map. We call the labels \emph{independent} iff we can define two sets of orthogonal labels $I_1'$ and $I_2'$ such that we can define separate maps:
\begin{enumerate}
\item $I_1' \times I_2' = I$
\item there exists a bijective map $f_1: I_1 \leftrightarrow I_1'$
\item there exists a bijective map $f_2: I_2 \leftrightarrow I_2'$
\item $f(i) = \langle f_1(i_1), f_2(i_2)\rangle$
\end{enumerate}
\end{defn}

\textbf{State counting}. We now look at how the cardinality of states evolves under a bijective map.

\begin{prop}\label{labelsCount}
Let $\hat{I} \subset I$ of finite size $n(\hat{I})$. Let $f(\hat{I})$ and $f^{-1}(\hat{I})$ be respectively the forward and backward image of a bijective map. We then have $n(\hat{I})=n(f(\hat{I}))=n(f^{-1}(\hat{I}))$, the number of states is conserved.
\end{prop}

\begin{prop}\label{labelsMultiCount}
Let $\hat{I_1} \subset I_1$ and $\hat{I_2} \subset I_2$ of finite size $n(\hat{I_1})$ and $n(\hat{I_2})$. Let $f(\hat{I_1})$, $f(\hat{I_2})$, $f^{-1}(\hat{I_1})$ and $f^{-1}(\hat{I_2})$ be the corresponding forward and backward images. We then have $n(\hat{I_1}\times\hat{I_2})=n(\hat{I_1})n(\hat{I_2})=n(f(\hat{I_1})) n(f(\hat{I_2}))=n(f^{-1}(\hat{I_1})) n(f^{-1}(\hat{I_2}))=n(\hat{f(I_1)}\times\hat{f(I_2)})=n(f^{-1}(\hat{I_1})\times f^{-1}(\hat{I_2}))$, the total number of states is conserved and remains the product of the number of labels in each set.
\end{prop}

Bijective maps preserve the number of states as they provide one-to-one association between future and past labels.

\section{Numeric labels}

\textbf{Discrete labels over $\mathbb{R}$}. Labels often will be numbers. For integers, we simply use $\mathbb{Z}$ as our set of labels. For real numbers, we need to update our definitions.

\begin{defn}\label{discreteLabelDef}
Consider a continuous numeric range. We divide the full range into contiguous cells. Let $I$ be the set of cells. For each cell we have a center value $x: I \mapsto \mathbb{R}$ and a width $w: I \mapsto \mathbb{R}$. $I$ is a set of \emph{discrete labels over a continuous range}.
\end{defn}

\begin{prop}\label{discreteLabelDist}
Each classical configuration state $\mathbbm{c}_j \in \mathbbm{C}_J$ can be identified by a distribution over states: $\mathbbm{c}_j=\sum D(i) \mathbbm{s}_i=\sum \rho(i) w(i) \mathbbm{s}_i$, where $D:I\rightarrow\mathbb{R}$ is defined as before, and $\rho(i)\equiv D(i) / w(i)$ is the density of the distribution for the cell. In physics terms, the distribution can be visualized as a histogram where $w(i)$, $\rho(i)$ and $D(i)$ are respectively the width, height and area of each bin.
\end{prop}

\begin{cor}\label{discreteLabelEv}
Let $f: I \leftrightarrow I$ be a bijective map. We have $\mathbbm{c}_{j'}=\sum D'(i) \mathbbm{s}_i=\sum \rho'(i) w(i) \mathbbm{s}_i = \sum D(f^{-1}(i)) \mathbbm{s}_i = \sum \rho(f^{-1}(i)) w(f^{-1}(i)) \mathbbm{s}_i$. $\rho'(i) = \rho(f^{-1}(i)) w(f^{-1}(i)) / w(i)$.
\end{cor}

The area moves from one cell of the histogram to the other. The height needs to be adjusted if the cell is of a different size.

\begin{defn}\label{discreteLabelHomogeneous}
A set $I$ of discrete labels over a continuous range is said to be \emph{homogeneous} if $w(i)=k$: the bins are of equal width.
\end{defn}

With homogeneous labels no adjustment is needed, and the range can be used as a measure of the number of labels.

\textbf{Continuous labels over $\mathbb{R}$}. We now make the bin width arbitrarily small.

\begin{defn}\label{continuousLabels}
A \emph{state variable} $X$ is the continuous limit of a set $I$ of discrete labels over a continuous range.
\end{defn}

To prepare for the limit we define\footnote{This $m(x)$ solves the same problems addressed by the invariant measure $m(x)$ introduced by Jaynes\cite{Jaynes}, which modifies Shannon's differential information entropy definition\cite{Shannon} to be invariant under coordinate transformation.} $m(i)=w(i)/\bar{w}$, where $\bar{w}$ represents the average width of the cells. We increase the number of the cells and reduce $\bar{w}$ while keeping $m(i)$ finite. In the limit we'll have a cell for each value, so we can use $x(i)$ (or simply $x$) instead of $i$ for the label. $\rho$ and $m$ will converge to functions defined over $x$. The corresponding configuration state will become $\mathbbm{c}_j=\int \rho(x) m(x) \mathbbm{s}_x dx$.

\begin{prop}\label{continuousMapping}
Let $f: X \leftrightarrow X$ be a bijective map on a state variable. The mapping must be continuous.
\end{prop}
Assume mapping is discontinuous at point $x$. Consider the cell at $x$ of width $m(x)dx$. The cell would be split into two, so it would not be mapped to one and only one cell. Therefore the mapping must be continuous.

\begin{prop}\label{widthMapping}
Let $f: X \leftrightarrow X$ be a bijective map on a state variable, and $x'=f(x)$. Then $dx' = \frac{m(x')}{m(x)} dx$. If $X$ is homogeneous, then $dx' = dx$; the range gives us a measure of the cardinality of labels and must be conserved.
\end{prop}
The mapping must be done so that the width of the cells is mapped as well, not just the center value. The width of the transported cell $m(x)dx \rightarrow m(x) dx'$ must be equal to the width of the target cell $m(x')dx$, which gives us the first part. If $X$ is homogeneous, $m(x)=m(x')=1$, which gives us the second part.

Let $\Delta x$ be a finite range of $X$. Before the limit, we have:
\begin{align*}
\Delta x = \frac{\Delta x}{1} = \frac{n(\Delta x) k}{n(1) k} = \frac{n(\Delta x)}{n(1)}
\end{align*}
where $n(\Delta x)$ and $n(1)$ are the number of labels/cells in the $\Delta x$ and unit range, while k is their width. The range can be seen as the ratio between the number of labels in the range and the number of labels in the unit range. That ratio must be conserved by the bijective map and remains well defined during the limit. In this sense, the range can be used as a measure of the cardinality of the labels.

\section{Single degree of freedom}

\begin{defn}\label{sdof}
A \emph{degree of freedom} is a label set $X$ given by a pair of homogeneous and orthogonal state variables $P$ and $Q$. We call these \emph{conjugate variables}.
\end{defn}

\begin{prop}\label{sdofMap}
Let $f: X \leftrightarrow X$ be a bijective map on a degree of freedom. Then $dq' \wedge dp' = dq \wedge dp$.
\end{prop}

This is the equivalent of \ref{widthMapping} for two state variables. The density $\rho(p,q)$ will be defined on cells of infinitesimal area proportional to $dq \wedge dp$. When mapping one cell to another, the infinitesimal area will remain the same. The area in phase space of one degree of freedom can then be used as a measure for the cardinality of the labels. \ref{labelsCount} becomes area conservation for conjugate variables. 

\begin{prop}\label{sdofInvariant}
Let $v$ and $w$ be two vectors defined on the tangent space of the manifold identified by two conjugate variables. Let
\begin{align*}
\omega_{\alpha, \beta} = \left[
  \begin{array}{cc}
    0 & 1 \\
    -1 & 0 \\
  \end{array}
\right] \\
\end{align*}
then $v'^{\alpha} \omega_{\alpha, \beta} w'^{\beta}=v^{\alpha} \omega_{\alpha, \beta} w^{\beta}$ under a bijective map.
\end{prop}

Area conservation is equivalent to requiring the invariance of the vector product, which is what $\omega_{\alpha, \beta}$ represents.

\begin{lem}\label{genAntisim}
Let $v$ and $w$ be two vectors. Let $v^{\alpha} \omega_{\alpha, \beta} w^{\alpha}$ be an antisymmetric product conserved under a continuous transformation parameterized by $t$. We can then define a function $H$ such that given $S^{\alpha} \equiv d_{t}x^{\alpha}$ and $S_{\beta} \equiv S^{\alpha} \omega_{\alpha, \beta}$, we have $S_{\alpha} = \partial_{\alpha}H$.
\end{lem}

$S^{\alpha}$ is the vector field that represents how the state variables change. Simply applying the vector transformation rules under continuous transformation we have:
\begin{align*}
v^{\alpha} \omega_{\alpha, \beta} w^{\beta} &= v'^{\alpha} \omega_{\alpha, \beta} w'^{\beta}  \\
&= (v^{\alpha} + \partial_{\gamma} S^{\alpha} dt v^{\gamma}) \omega_{\alpha, \beta} ( w^{\beta} + \partial_{\delta} S^{\beta} w^{\delta} dt) \\
&= v^{\alpha} \omega_{\alpha, \beta} w^{\beta} + (\partial_{\gamma} S^{\alpha} v^{\gamma} \omega_{\alpha, \beta} w^{\beta} \\
 &+ v^{\alpha} \omega_{\alpha, \beta} \partial_{\delta} S^{\beta} w^{\delta}) dt + O(dt^2)
\end{align*}
\begin{align*}
v^{\gamma} w^{\beta} \partial_{\gamma} S_{\beta} - v^{\alpha} w^{\delta} \partial_{\delta} S_{\alpha} = 0
\end{align*}
\begin{align*}
\partial_{\alpha} S_{\beta} - \partial_{\beta} S_{\alpha} &= curl(S_{\alpha}) = 0 \\
S_{\alpha} &= \partial_{\alpha}H
\end{align*}

\begin{prop}\label{sdofHam}
The evolution for a single degree of freedom is given by:
\begin{align*}
d_{t}q &= \partial_{p} H \\
d_{t}p &= - \partial_{q} H
\end{align*}
\end{prop}

Simply expand \ref{genAntisim} with the metric defined in \ref{sdofInvariant}. We recognize Hamilton's equations for one degree of freedom\cite{classical_dynamics}.

\section{Multiple degrees of freedom}

\begin{defn}\label{mdof}
Two degrees of freedom are said to be \emph{independent} if the corresponding label sets $X^1$ and $X^2$ are independent.
\end{defn}

\begin{prop}\label{mdofInvariant}
Let $v$ and $w$ be two vectors defined on the tangent space of the manifold identified by two independent degrees of freedom. Let $\alpha$ and $\beta$ be indexes for the state variables $q^1, p^1, q^2, p^2$. Let
\begin{align*}
\omega_{\alpha, \beta} =  \left[
  \begin{array}{cc}
    1 & 0 \\
    0 & 1 \\
  \end{array}
\right] \otimes \left[
  \begin{array}{cc}
    0 & 1 \\
    -1 & 0 \\
  \end{array}
\right] =
\left[
  \begin{array}{cccc}
    0 & 1 & 0 & 0 \\
    -1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & -1 & 0 \\
  \end{array}
\right] \\
\end{align*}
then $v'^{\alpha} \omega_{\alpha, \beta} w'^{\beta}=v^{\alpha} \omega_{\alpha, \beta} w^{\beta}$ under a bijective map.
\end{prop}

The orthogonality of the labels corresponds to orthogonality in phase space, which the mapping needs to preserve across independent degrees of freedom. The cardinality of labels within each degree of freedom corresponds to the area in phase space\footnote{We assume we are using the same unit across d.o.f.}, which the mapping also needs to preserve.\footnote{These statements provide a direct physical interpretation for Gromov's non-squeezing theorem\cite{Gromov,deGosson,Stewart}.} This is equivalent to requiring the conservation of the scalar product across independent degrees of freedom, while still requiring conservation of the vector product within. That leads us to the metric defined by \ref{mdofInvariant}.
The metric generalizes \ref{sdofInvariant} to give us the cardinality of labels defined on the area given by two arbitrary vectors. For an infinitesimal region, this corresponds to $dq^1 \wedge dp^1 + dq^2 \wedge dp^2$, the sum of the projections on the independent planes. Moreover, because of \ref{labelsMultiCount}, the product of the areas of each d.o.f. is also conserved; the volume of phase space corresponds to the cardinality of states, which a bijective map conserves: this is the Louisville's theorem.

\begin{prop}\label{mdofHam}
The evolution for multiple degrees of freedom is given by:
\begin{align*}
d_{t}q^i &= \partial_{p^i} H \\
d_{t}p^i &= - \partial_{q^i} H
\end{align*}
\end{prop}

Expand \ref{genAntisim} with the metric defined in \ref{mdofInvariant}. We recognize Hamilton's equations for multiple degrees of freedom\cite{classical_dynamics}.

\section{Time dependence}
So far we have assumed that neither state labeling nor mapping change in time. If they do, we need to to use time as a label and therefore introduce an extra degree of freedom.

\begin{defn}\label{tdof}
The \emph{temporal degree of freedom} is a label set $X$ given by the pair of conjugate variables $T$ and $E$. We call \emph{extended phase space} the outer product between phase space and the temporal degree of freedom.
\end{defn}

\begin{prop}\label{tdofMonotonic}
Let $s$ be the parameter of a trajectory in the extended phase space of a deterministic and reversible system. The trajectory must be continuous. There must exist a strictly monotonic function $t(s)$.
\end{prop}

The trajectory has to be continuous in both standard and temporal variables because of \ref{continuousMapping}. Since determinism and reversibility are defined in time, the trajectory must traverse all times once and only once: we must have an invertible mapping between $t$ and $s$, which means we must have a strictly monotonic $t(s)$.

\begin{defn}\label{tdofAntistates}
We call \emph{standard states} those connected by a trajectory where $d_{s}t>0$. We call \emph{anti-states} those connected by a trajectory where $d_{s}t<0$.
\end{defn}

Since $t(s)$ is strictly monotonic, $d_{s}t$ along a trajectory cannot change sign, so we have the division between particle and anti-particle states. Note that since the parametrization is conventional and can be changed to $s'=-s$, what we call particle and anti-particle states is also conventional. What is physical and not conventional, though, is that particle and anti-particle states cannot be connected by deterministic and reversible evolution.

\begin{prop}\label{tdofInvariant}
Let $v$ and $w$ be two vectors defined on the tangent space of the manifold identified by the temporal degree of freedom and one standard degree of freedom. Let $\alpha$ and $\beta$ be indexes for the state variables $t, e, q, p$. Let
\begin{align*}
\omega_{\alpha, \beta} =  \left[
  \begin{array}{cc}
    -1 & 0 \\
    0 & 1 \\
  \end{array}
\right] \otimes \left[
  \begin{array}{cc}
    0 & 1 \\
    -1 & 0 \\
  \end{array}
\right]
= \left[
  \begin{array}{cccc}
    0 & -1 & 0 & 0 \\
    1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & -1 & 0 \\
  \end{array}
\right] \\
\end{align*}
then $v'^{\alpha} \omega_{\alpha, \beta} w'^{\beta}=v^{\alpha} \omega_{\alpha, \beta} w^{\beta}$ under deterministic and reversible evolution.
\end{prop}

$\langle T, E \rangle$ are not independent labels from $\langle Q, P \rangle$ as they do not define new states. So they are not necessarily orthogonal in the extended phase space. Looking back at \ref{discreteLabelDef}, cells need to be defined on the plane where $\langle Q, P \rangle$ (maximally) change: this is not the plane of constant $\langle E, T \rangle$ (they are not orthogonal) where $dq \wedge dp$ is defined, but the plane perpendicular to constant $\langle Q, P \rangle$ where $dt \wedge de$ is defined. On that plane we can properly count states and define our invariant.

We have a right triangle-like relationship between the plane where the invariant is defined and its projections on the planes defined by each d.o.f., similar to the multiple d.o.f.:
\begin{align*}
m.d.o.f \;\;\; &dq^1 \wedge dp^1 + dq^2 \wedge dp^2 = k \\
t.d.o.f \;\;\; &dt \wedge de + k = dq \wedge dp \\
\end{align*}
But in the previous case, the right angle was between the two independent d.o.f.. In this case, the right angle is between the invariant and the plane of constant $\langle Q, P \rangle$ where $dt \wedge de$ is defined. We rewrite it as $dq \wedge dp - dt \wedge de = k$. This corresponds to the Minkowski product across d.o.f. and the vector product within. The metric, with a space-like convention, still gives us the cardinality of labels within a degree of freedom.\footnote{Adjusted to avoid double counting.}

\begin{prop}\label{tdofHam}
The evolution for time varying multiple degrees of freedom is given by:
\begin{align*}
d_{s}t &= - \partial_{e} \mathcal{H} \\
d_{s}e &= \partial_{t} \mathcal{H} \\
d_{s}q^i &= \partial_{p^i} \mathcal{H} \\
d_{s}p^i &= - \partial_{q^i} \mathcal{H}
\end{align*}
\end{prop}

Take the metric from \ref{tdofInvariant}, add multiple independent d.o.f as in \ref{mdofInvariant}, use \ref{genAntisim} with the parameter $s$ instead of $t$ and generator $\mathcal{H}$ instead of $H$.

If we set\footnote{We avoided using $p^{n+1}$ as it hides the minus sign from the metric, making it seem that the temporal d.o.f is just another independent d.o.f.} $q^{n+1}=t$ and $p^{n+1}=-e$, we recognise Hamilton equations in the extended phase space\footnote{As in Struckmeier\cite{Struckmeier}, $d_{s}t$ need not be unitary.}\cite{Synge,Lanczos}.

It should not be a surprise that the equations do not mention the speed of light $c$. In fact, nothing says that all $q^i$ represent space or that the laws of motion are invariant in all inertial frames. The only requirement we have is that the areas of each degrees of freedom represent the same cardinality for labels.

\begin{prop}\label{tdofConstrain}
The evolution is constrained by $\mathcal{H}=k$.
\end{prop}

Since $\mathcal{H}$ is constant through the evolution, it can serve both as the generating function and as the evolution constraint. By convention, we could set $\mathcal{H}=0$ without loss of generality as changing $\mathcal{H}$ by a constant does not change the equation of motion. This reduces extended phase space to $\mathbb{R}^{2*N + 1}$, the state variables plus time.

\textbf{Example}. We wrap up with an example. Let $\mathcal{H} = mc^2 + ((p^i)^2 - e^2/c^2) / 2m$ = 0.  If we apply \ref{tdofHam} we have:
\begin{align*}
d_{s}t &= e / mc^2 \\
d_{s}e &= 0 \\
d_{s}q^i &= p^i / m \\
d_{s}p^i &= 0
\end{align*}
For standard states (positive $e$), let $s=\tau$:
\begin{align*}
e / c &= m c d_{\tau}t = m U^0 = P^0 \\
p^i &= m d_{\tau}q^i = m U^i = P^i \\
\end{align*}
so we recognize the four-momentum $P^\alpha = [e/c, p^i]$ and $\tau$ proper time. For anti-states (negative $e$), let $s=-\tau$:
\begin{align*}
- e / c &= m c d_{\tau}t = m U^0 = P^0 \\
- p^i &= m d_{\tau}q^i = m U^i = P^i \\
\end{align*}
we end up with a minus sign between the four-momentum and the conjugate variables $P^\alpha = [-e/c, -p^i]$.\footnote{This is consistent with the Dirac field where we have $[i\hbar\partial_t, -i\hbar\partial_{x^i}]$ for generators and $\psi^\dagger\gamma^0[i\hbar\partial_t, -i\hbar\partial_{x^i}]\psi$ for observables, and $\gamma^0$ is $1$ for particles and $-1$ for anti-particles.} $\mathcal{H}$ relates to the rest energy, the parametrization to proper time (its conjugate). Parametrization and time are aligned for standard states and anti-aligned for anti-states.

\section{Conclusion}
By deriving Hamiltonian mechanics from simple definitions of labels, states, determinism and reversibility we have given more direct physical meaning to phase space and its geometric properties, and shown that a good part of the Hamiltonian framework can stand on its own, without Lagrangians. The further emergence of the Minkowsky metric and the distinction between standard and anti-states by the inclusion of the temporal d.o.f. is also noteworthy. No mathematical breakthrough is revealed here, yet I am not aware of any work that brings all the pieces of the puzzle together in quite this way: so much derived from so little.

The hope is that, by continuing in this approach, we can shed more light on why the laws of physics are what they are; and show that they are not arbitrary rules, but necessary given few simple assumptions.

\begin{thebibliography}{5}

\bibitem{Jaynes} Jaynes, E. T.: ``Information theory and statistical mechanics'', (1963)
\bibitem{Shannon} Shannon, C. E.: ``A mathematical theory of communications'', The Bell System Technical Journal, Vol. 27, pp. 379–423, 623–656, (1948)
\bibitem{classical_dynamics} J. V. Jose', E. J. Saletan: ``Classical Dynamics'', Cambridge University Press, (1998)
\bibitem{Gromov} Gromov, M. L.: ``Pseudo holomorphic curves in symplectic manifolds''. Inventiones Mathematicae 82: 307–347, (1985)
\bibitem{deGosson} de Gosson, M. A.: ``The symplectic camel and the uncertainty principle: the tip of an iceberg?'', Foundation of Physics 39, pp. 194–214, (2009)
\bibitem{Stewart} Stewart, I.: ``The symplectic camel'', Nature 329(6134), 17–18 (1987)
\bibitem{Lanczos} Lanczos, C.: ``The variational principles of mechanics'', University of Toronto Press (1949)
\bibitem{Synge} Synge, J. L.: Encyclopedia of Physics Vol 3/1, Springer (1960)
\bibitem{Struckmeier} Struckmeier, J.: ``Hamiltonian dynamics on the symplectic extended phase space for autonomous and non-autonomous systems'', J. Phys. A: Math. Gen 38, 1257-1278, (2005)

\end{thebibliography}

\end{document}
