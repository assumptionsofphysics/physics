\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}

\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsthm}

\newtheorem{assump}{Assumption}
\renewcommand*{\theassump}{\Roman{assump}}
\newtheorem{prop}{Proposition}[section]

\theoremstyle{definition}
\newtheorem{defn}[prop]{Definition}

\newenvironment{rationale}{\emph{Rationale}.}{$\Box$}
\newenvironment{justification}{\emph{Justification}.}{$\Box$}
\renewenvironment{proof}{\emph{Proof}.}{$\Box$}

\begin{document}

\title{From physical principles to ???}
\author{Gabriele Carcassi}
\affiliation{University of Michigan, Ann Arbor, MI 48109}
\email{carcassi@umich.edu}
\date{March 15, 2015}

\begin{abstract}
Deterministic/reversible assumption. Describe fundamental model of physics. System is always interacting with environment, both deterministically and non-deterministically. Deterministic means only depends on the state of the system. Set of states needs to be invariant under non-deterministic interaction: the non-deterministic interaction affects what are the states of the system. Deterministic/reversible evolution is necessary to even be able to talk about a system (trivial deterministic process).

Define: set of states, deterministic evolution as permutation, labels/state variables, cardinality of state variable, state id function (=1 for a particular state). Can we do for continuous variables as well? Can we find divergence free evolution for det/rev and independent variables?

Infinite reducibility assumption. Decomposable systems. Rule of composition. Inverse and null state to study changes. Set of transformations to increase and decrease the quantity in each state. Define magnitude of the system (probably needed to guarantee things converge in math?).

Define: set of states as a vector space, inner product.

Continuous quantities. Need for densities to be a function of the state (not state variable). Defined on infinitesimal interval. Define: degree of freedom, state cardinality, metric (T*Q).
\end{abstract}
\maketitle

\section{Introduction}

Classical particle mechanics is usually founded on Newton's laws. These, though, are insufficient to derive the full Lagrangian and Hamiltonian formalism, and usually other ad-hoc assumptions (e.g. conservative forces) are introduced. Special relativity is based on two principles (invariance of the speed of light and the principle of relativity), which lead to the Minkowskian nature of space-time but not to the equations of motion, which are a consistent reformulation of the non-relativistic ones. Quantum mechanics is simply founded on its mathematical formulation, leaving its physical significance open to debate. One can't come to the conclusion that, unfortunately, the landscape of fundamental physics is a piece-meal of concepts and idea, sometime apparently inconsistent with each other. Yet: the physical world is one, and our physical theories are simply models of such reality. If we clarify what those models represent, and on what assumptions are based, shouldn't we be able to have a coherent and more satisfying picture?

This work aims to re-organize the known elements and equations in a more consistent and comprehensive way, leading to better insight on why the fundamental concepts and laws are what they are. We will start with physical assumptions, that clarify the models we impose on the physical world, and give arguments on when those assumptions can be considered valid. We will justify our mathematical definitions, based on the formal properties of the objects of our discussion. And will, as a consequence, re-derive known results and theories. We will strive to do this in a way that is mathematically meaningful, philosophically consistent and mathematically precise.

TODO: high level view of the derivation

%We'll use concepts from different disciplines, such as set theory, differential geometry, relativity, Hamiltonian and Lagrangian mechanics, and we'll find interesting connections among them. We'll keep names and notation as consistent as possible to current use across the different disciplines. This may sometimes lead to some non sequitur as it will not be immediately clear why the new definitions are equivalent to the standard ones. These are typically resolved by subsequent derivation of the expected properties.

%No mathematical breakthrough should be expected: the goal, after all, is to derive the \emph{known} framework from a set of \emph{simple} definitions in the most \emph{obvious} way possible. No proof is longer than a couple of paragraphs, so the word \emph{theorem} is avoided in favor of \emph{proposition} and \emph{corollary}. The novel, and surprising, result is how so much can be derived from so little.

\section{About}

The work is organized into:
\begin{description}
  \item[Assumptions] these characterize the physical system we are studying and constitute the premise of our discussion. A \textbf{rationale} follows each assumption, which uses physical and sometimes philosophical arguments to motivate why (or why not) such an assumption makes sense (in a particular case).
  \item[Definitions] these encode into mathematical language the physical concepts we are studying. A \textbf{justification} follows each definition, which uses physical and mathematical arguments to explain why such a definition and its properties are necessary to describe a particular physical concept.
  \item[Propositions] these are statements that mathematically follow from definitions or other propositions. A mathematical \textbf{proof} follows each proposition. No mathematical breakthrough should be expected as we mostly use well known results from different fields. The goal is to show how those results make sense in light of the physical assumptions we start with.
\end{description}
This should allow you, the reader, to focus on the parts you are most interested in and skim the rest. The philosophically inclined may focus on the assumptions, their rationale and the definitions. The mathematically inclined may focus on the definitions, the propositions and their proofs. The scientist on the assumptions, the definitions and their justifications, and the propositions.

\section{Determinism and reversibility}

%TODO: it would be nice to have "examples" always refer to the same cases, and that they pertain to different type of physics. E.g. trajectory, gravity, thermodynamics, fluid dynamics, biology, electromagnetism...

We start by fixing a \emph{physical system} we want to study, that is something we can interact with and perform measurements on (e.g. a cat, a planet, water). We call \emph{environment} everything else. We set what particular aspect we want to study (e.g. the trajectory when falling, the motion around a star, its flow in a pipe). We call \emph{state} a particular configuration in time of the portion under study. Since the state does not, in general, exhaust the description of the system, a part remains \emph{unstated}, and as such we'll call it, for lack of a better word. With this in mind, we introduce the following:

\begin{assump}[Determinism and reversability]
The state of the physical system under study undergoes deterministic and reversible evolution.
\end{assump}

By deterministic (and reversible) we mean that future (and past) states are uniquely identified by the present state. We can then think of the state of the system as the part that interacts deterministically and reversibly with the environment (and itself) while the unstated part as interacting non-deterministically with the environment (and itself). We call this setup the \emph{fundamental model of physics}, as it is the simplest model that still captures the crucial aspects of studying and writing laws for a physical system.

Much of the focus later will be given to the state under deterministic and reversible evolution. However, the non-deterministic part plays a fundamental role as well. In fact: it is this aspect of the evolution that determines what states are available and their description. Suppose we study the motion of a small particle, its state under gravitational and inertial forces will be properly described by the position and momentum of the center of mass. Suppose we study the same small particle as suspended in a fluid, as it undergoes Brownian motion: its state will be a probability distribution for position and momentum of the center of mass. Gravitational and inertial forces have not changed, yet the set of states have. The issue is that the set of states must be closed under the non-deterministic evolution as well. That is: if we apply the non-deterministic evolution to any state, we still need to end up in one of the possible states. If the Brownian motion is strong enough, the state gets knocked out of the set of states determined by a simple position/momentum pair.

A similar more drastic effect: consider a book and its motion under gravitation and inertial forces, its state being the position and momentum of the center of mass. As we increase the temperature of the air around the book, its motion remains unaffected until, at some point, the book burns. Clearly, the non-deterministic evolution has brought one of the states outside the set, to the point that the system is no longer recognizable.

As we have seen, sometimes the state is identified by a distribution (either statistical or actual). Even in this case, the state can be deterministic and reversible. That is: given the distribution at one time we can determine the distribution at future times. The shape and the parameters of the distribution can be deterministic, even if the individual trajectories are not (those fall within the unstated part). In fact: we cannot assume trajectory and states are defined at all for the unstated part. Consider a muon and its decay products: it is clear that the three outgoing particles have a state and trajectory of their own, it is clear that the resulting total mass and energy came from the muon. Yet, before the decay, we cannot ascribe an internal state and trajectory to those parts. The state of a muon is not some combination of the state of an electron and two neutrinos.

Now that we have clarified what we mean by deterministic and reversible evolution (even on a statistical ensemble), and how also the non-deterministic part is key to define states (as the set as a whole needs to be invariant under it), we can ask ourselves: when is this assumption valid?

\begin{rationale}
The claim is that deterministic and reversible evolution is necessary to be able to study a physical system. That is: the only part of a system that can be studied through reliable and reproducible experimentation or observation is the part that undergoes deterministic and reversible evolution. We can provide different arguments that point in the same direction.

First, to be able to identify the system, we must first be able to tell it apart from the environment. Intuitively, we can distinguish between two chairs because we can move the first to another room and sit on it without having touched the second. We can manipulate the state of the first system without affecting the second, and vice-versa. So, to tell a system apart from its environment, there has to to be a certain amount of isolation between the two: if I affect its state, the state of the other systems is not affected and vice-versa. This means that the system future and past states are only determined by its own state and the state undergoes deterministic and reversible evolution.

Second, the aim of physics is to write laws that can be used to make prediction that can be validated experimentally. If I drop an anvil from a tower, it will accelerate at $9.81 m/s^2$; if I want the anvil to reach the ground at $x m/s$ I have to drop it from $y m$. To the extent that we want to make predictions in time, we need to have a one-to-one correspondence between initial and final states.

Third, operationally we must reliably prepare and measure states. That is, we need a process for which the input settings of our preparing device determine the outgoing state of the system; and a process for which the incoming state of the system can be reconstructed by the output of the measuring device. That is, our system must participate in a deterministic and reversible process with the preparing and measuring device. How else could we calibrate our experimental apparatus?

This link between state definition and deterministic processes should not be too surprising as the state, in the context of thermodynamics and system theory, is often defined as \emph{the set of variables needed to determine the future evolution of the system}. As we saw before, this applies also to statistical processes: the distribution (the ensemble) as a whole can be indeed calculated, measured and prepared, not each individual element.

So, if it seems we need to require deterministic and reversible motion, why we call this an assumption? First of all, because it's an idealization: it can never be completely achieved in practice. A system can be prepared or measured up to a certain level of precision. Perfect isolation of a system is impossible both practically (e.g. black-body radiation, gravity, ...) and conceptually (e.g. if the system is perfectly isolated, we cannot interact with it: how can it studied through experimentation?). It's a simplifying assumption that can only be taken if the environment and the internal dynamics of the system interact in such a way that they affect and are little affected by the aspect we are studying. As we saw before, for example, assuming that the states consists of the position and momentum of the center of mass means assuming that the Brownian motion of the body is negligible.

The second reason is that technically the assumption comes first. If we start looking for a law, we are implicitly assuming that there exists one to be found. The opposite assumption, there is no law to be found, can't really be experimentally proven exhaustively (i.e. you can only show that you have not found it among a certain class). Very often the hard part in science is finding for what system and in what conditions such assumption hold. Finding the law after that is a much simpler endeavour, and is something that is routinely done by students in their intro labs.
\end{rationale}

We are now ready to capture the elements of our discussion and their properties through mathematical definitions.

\begin{defn}\label{statedef}
Fix a physical system for study. There exists a set $\mathfrak{S}$, which we call \emph{state space}, consisting of all the physically distinguishable configurations $\mathfrak{s}$, which we call \emph{states}. There exists a bijective map $f_{\Delta t}:\mathfrak{S} \leftrightarrow \mathfrak{S}$, which we call \emph{time evolution operator}, that links initial and final states.
\end{defn}

\begin{justification}
The collection of all possible states from a set. The deterministic and reversible evolution maps one element in the set to one and only one element of the set.
\end{justification}

In practice, we use physical quantities to identify states. For example, the state for an ideal gas is given by pressure, volume and temperature. To clarify whether we are referring to a particular value of such quantity, or the quantity itself, we introduce the following terminology:
\begin{center}
    \begin{tabular}{ | p{2.5cm} | p{5.5cm} | }
    \hline
    State variable & A quantity (discrete or continuous) that must be specified to identify a state (e.g. position). \\ \hline
    Label & A particular value for a state variable (e.g. position = $5m$). \\ \hline
    Label range & A set of possible values for a state variable (e.g. position = $[4.5m, 5.5m]$). \\
    \hline
    \end{tabular}
\end{center}

\begin{defn}\label{label}
Let $\mathbbm{I}$ be a set. Let $f_\mathbbm{I} : \mathfrak{S} \rightarrow \mathbbm{I}$ be a function that maps each state to an element of the set. We call the pair $(\mathbbm{I}, f_\mathbbm{I})$ a \emph{state variable}. We call the pair $(i, f_\mathbbm{I}^{-1}(i))$ a \emph{label}, where $i \in \mathbbm{I}$ and $f_\mathbbm{I}^{-1}(i) = \{\mathfrak{s} \in \mathfrak{S} | f_\mathbbm{I}(\mathfrak{s}) = i\}$ is the reverse image of $i$. We call the pair $(I, f_\mathbbm{I}^{-1}(I))$ a \emph{label range}, where $I \subset \mathbbm{I}$.
\end{defn}

\begin{justification}
A state variable associates each state to one of its values. A label, one of the possible values, can also be thought as representing all states that are identified by its value. A label range, a set of possible values, can also be thought as representing all states that are identified by those values. Abusing the notation, $\mathbbm{I}$ will represent both the set of possible values and the function, and $i$ both the possible value and the set of states.
\end{justification}

TODO: labels/state variables, cardinality of state variable, state id function (=1 for a particular state). Can we do for continuous variables as well? Can we find divergence free evolution for det/rev and independent variables?

\section{Composite systems and reducibility}

We know introduce the idea that a system is decomposable, that is it is made of smaller parts, and we characterize the relationship between the whole and the parts with the following assumption:

\begin{assump}[Infinite reducibility or Classical]\label{classical}
The system is infinitely reducible: it can be thought of as composed of two or more similar but smaller systems, each in its own configuration state, which in turn can be thought of as composed of two or more, ad infinitum. We call \emph{particle} such an infinitesimal part.
\end{assump}

\begin{rationale}
This makes a clear demarcation of what a classical object is (and conversely what a quantum object is not): it is something that can be fully described up to its infinitesimal constituents. This is clearly a \emph{simplifying} assumption, and it is instructive to understand when and why it cannot be used.

The first problem is methodological. As we saw before, we need access to a deterministic and reversible process to be able to study a system and define a state. For two balls, we can imagine studying pieces of the balls in isolation, and then describe the collision between the two by describing what happens at each piece. The classical assumption holds. For an electron and a photon, we cannot take pieces of the electron or the photon, study them in isolation and then describe how each part moves during Compton scattering. The classical assumption does not hold: we do not have suitable physical processes at our disposal.

The second problem is more conceptual. As we saw before, a system cannot be fully isolated from the environment. The smaller the part we study, the more it will be susceptible to fluctuations from the environment, the less we can justify the deterministic assumption, the less we are in a position to define its state. While the overall position of a ball can be considered fairly unaffected by its surrounding, the position of each small piece is going to be constantly affected by air and light scattering on its surface.
\end{rationale}

TODO: 

We are now ready to capture the elements of our discussion and their properties through mathematical definitions.

\begin{defn}\label{vector space}
The state space $\mathfrak{C}$ for a infinitesimally reducible system is a vector space. It admits a basis $\mathfrak{S}$ that represents the state space for the infinitesimal parts.
\end{defn}

\begin{justification}
We first claim that the state space $\mathfrak{C}$ is an additive monoid: there exist a law of composition $+ : \mathfrak{C} \times \mathfrak{C} \rightarrow \mathfrak{C}$ that takes two states and returns one that is the physical composition of the two. The domain and codomain match because the system is homogeneous. The law is commutative $\mathfrak{c}_1 +\mathfrak{c}_2 = \mathfrak{c}_2+\mathfrak{c}_1$ and associative $(\mathfrak{c}_1 + \mathfrak{c}_2) + \mathfrak{c}_3 = \mathfrak{c}_1 + (\mathfrak{c}_2 + \mathfrak{c}_3)$, as it does not matter in what order we compose the parts. There exist a zero element $\mathfrak{c} + 0 = \mathfrak{c}$ and it represent the state with no system.

We then claim it is an additive group. As we want to describe changes during the evolution, i.e. $\Delta \mathfrak{c}$, we introduce an inverse $\mathfrak{c}_2$ for each element $\mathfrak{c}_1$ such that $\mathfrak{c}_1 + \mathfrak{c}_2 = 0$. Such inverse may not represent a true physical state, yet we'll still call $\mathfrak{C}$ the state space committing an abuse of terminology. Having an inverse, $\mathfrak{C}$ is an additive (i.e. abelian) group.

We now consider the set of transformations $A$ that increase or decrease the size of the system by a constant. This forms a field\footnote{Here field is intended in the abstract algebraic sense (a nonzero commutative division ring) which has no relationship to the field in the physics sense (a physical quantity with a value for each point in space).} that is isomorphic to $\mathbbm{R}$. In fact, let $a: \mathbbm{R} \rightarrow A$ be a mapping between a number and the transformation that increases or decreases the size of the system by that number; define on $A$ an addition $+: A \times A \rightarrow A$ and a multiplication $*: A \times A \rightarrow A$ such that $a(x) + a(y) = a(x+y)$ and $a(x) * a(y) = a(x*y)$, so that the sum and product of the transformation is equal to the sum and product of their respective factors. $A$ is a field, as its sum and multiplication have all properties of $\mathbbm{R}$ which is a field. $a$ is an homomorphism by definition and is invertible by the very definition of $A$, each element fully identified by its constant. Therefore $A$ and $\mathbbm{R}$ are isomorphic.

We now claim that the state space $\mathfrak{C}$ is a vector space over $\mathbbm{R}$. The abelian group $\mathfrak{C}$ can be extended with the operations defined by $A$, as each element $a \in A$ is a map $a : \mathfrak{C} \rightarrow \mathfrak{C}$. The map has the following properties: $(a_1 + a_2) \mathfrak{c} = a_1 \mathfrak{c} + a_2 \mathfrak{c}$, increasing the size of the system by the sum of two constant is the same as combining the separate increases, and $a (\mathfrak{c}_1 + \mathfrak{c}_2) = a \mathfrak{c}_1 + a \mathfrak{c}_2$, increasing the size of the total system is the same as the combination of the increased parts. $\mathfrak{C}$ is a module over $A$, which is a field and isomorphic to $\mathbbm{R}$. We can then think of $\mathfrak{C}$ as a vector space over $\mathbbm{R}$.
\end{justification}


\begin{thebibliography}{0}

\bibitem{Shannon} Shannon, C. E., ``A mathematical theory of communication'', The Bell System Technical Journal, Vol. 27, pp. 379--423, 623--656, (1948).
\bibitem{Jaynes} Jaynes, E. T., ``Information theory and statistical mechanics'', Statistical Physics 3, pp. 181--218, (1963).
\bibitem{classical_dynamics} J. V. Jos\'{e}, E. J. Saletan, ``Classical Dynamics'', Cambridge University Press, (1998).
\bibitem{Gromov} Gromov, M. L., ``Pseudo holomorphic curves in symplectic manifolds'', Inventiones Mathematicae 82, pp. 307--347, (1985).
\bibitem{deGosson} de Gosson, M. A., ``The symplectic camel and the uncertainty principle: the tip of an iceberg?'', Foundations of Physics 39, pp. 194--214, (2009).
\bibitem{Stewart} Stewart, I., ``The symplectic camel'', Nature 329, pp. 17--18, (1987).
\bibitem{Lanczos} Lanczos, C., ``The variational principles of mechanics'', University of Toronto Press, (1949).
\bibitem{Synge} Synge, J. L., ``Classical dynamics'', Encyclopedia of Physics Vol 3/1, Springer (1960).
\bibitem{Struckmeier} Struckmeier, J., ``Hamiltonian dynamics on the symplectic extended phase space for autonomous and non-autonomous systems'', J. Phys. A: Math. Gen. 38, pp. 1257--1278, (2005).

\end{thebibliography}

\end{document}
