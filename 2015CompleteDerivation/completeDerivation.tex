\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}

\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{dutchcal}

\newtheorem{assump}{Assumption}
\renewcommand*{\theassump}{\Roman{assump}}
\newtheorem{prop}{Proposition}[section]

\theoremstyle{definition}
\newtheorem{defn}[prop]{Definition}

\newenvironment{rationale}{\emph{Rationale}.}{\qed}
\newenvironment{justification}{\emph{Justification}.}{\qed}
\renewenvironment{proof}{\emph{Proof}.}{\qed}

\begin{document}

\title{From physical principles to ???}
\author{Gabriele Carcassi}
\affiliation{University of Michigan, Ann Arbor, MI 48109}
\email{carcassi@umich.edu}
\date{March 15, 2015}

\begin{abstract}
Deterministic/reversible assumption. Describe fundamental model of physics. System is always interacting with environment, both deterministically and non-deterministically. Deterministic means only depends on the state of the system. Set of states needs to be invariant under non-deterministic interaction: the non-deterministic interaction affects what are the states of the system. Deterministic/reversible evolution is necessary to even be able to talk about a system (trivial deterministic process).

Define: set of states, deterministic evolution as permutation, labels/state variables, cardinality of state variable, state id function (=1 for a particular state). Can we do for continuous variables as well? Can we find divergence free evolution for det/rev and independent variables?

Infinite reducibility assumption. Decomposable systems. Rule of composition. Inverse and null state to study changes. Set of transformations to increase and decrease the quantity in each state. Define magnitude of the system (probably needed to guarantee things converge in math?).

Define: set of states as a vector space, inner product.

Continuous quantities. Need for densities to be a function of the state (not state variable). Defined on infinitesimal interval. Define: degree of freedom, state cardinality, metric (T*Q).
\end{abstract}
\maketitle

\section{Introduction}

While some branches of fundamental physics (e.g. Newtonian mechanics and special relativity) are founded on physical laws or principles others (e.g. Lagrangian/Hamiltonian/Quantum mechanics) start by postulating mathematical frameworks or relationships. The latter approach presents a significant drawback: the mathematical structure is not enough to define the physical system it describes. We are left asking: under what physical conditions a particular system is described by Lagrangian (or Quantum) mechanics? Naturally one \emph{can} define a Lagrangian system (or a Quantum system) as one that obeys those rules, but that begs the question. Another issue is that each element of the mathematical structure may or may not correspond to some physical concept. Suppose the earning (or losses) of two companies are governed by Hamiltonian's equations, which variable is on the cotangent space of the other? In other words: if we want to fully appreciate what the fundamental theories of physics are describing, what each mathematical concept corresponds to in the physical world, we need to start by characterizing the physical system we are describing in a precise enough way that it can be encoded in mathematical language. Naturally, the physical insight we gain will be relevant for only that type of system (i.e. other different systems may \emph{happen} to use the same equations), but what we'll see is that the assumptions we make are minimal and can apply to a variety of systems, especially the ones that are considered fundamental. It is indeed surprising how so much can be derived by imposing so little.

TODO: high level view of the derivation


%This work aims to re-organize the known elements and equations in a more consistent and comprehensive way, leading to better insight on why the fundamental concepts and laws are what they are. We will start with physical assumptions, that clarify the models we impose on the physical world, and give arguments on when those assumptions can be considered valid. We will justify our mathematical definitions, based on the formal properties of the objects of our discussion. And will, as a consequence, re-derive known results and theories. We will strive to do this in a way that is mathematically meaningful, philosophically consistent and mathematically precise.


%We'll use concepts from different disciplines, such as set theory, differential geometry, relativity, Hamiltonian and Lagrangian mechanics, and we'll find interesting connections among them. We'll keep names and notation as consistent as possible to current use across the different disciplines. This may sometimes lead to some non sequitur as it will not be immediately clear why the new definitions are equivalent to the standard ones. These are typically resolved by subsequent derivation of the expected properties.

%No mathematical breakthrough should be expected: the goal, after all, is to derive the \emph{known} framework from a set of \emph{simple} definitions in the most \emph{obvious} way possible. No proof is longer than a couple of paragraphs, so the word \emph{theorem} is avoided in favor of \emph{proposition} and \emph{corollary}. The novel, and surprising, result is how so much can be derived from so little.

\section{About}

The work is organized into:
\begin{description}
  \item[Assumptions] these characterize the physical system we are studying and constitute the premise of our discussion. A \textbf{rationale} follows each assumption, which uses physical and sometimes philosophical arguments to motivate why (or why not) such an assumption makes sense (in a particular case).
  \item[Definitions] these encode into mathematical language the physical concepts we are studying. A \textbf{justification} follows each definition, which uses physical and mathematical arguments to explain why such a definition and its properties are necessary to describe a particular physical concept.
  \item[Propositions] these are statements that mathematically follow from definitions or other propositions. A mathematical \textbf{proof} follows each proposition. No mathematical breakthrough should be expected as we mostly use well known results from different fields. The goal is to show how those results make sense in light of the physical assumptions we start with.
\end{description}
This should allow you, the reader, to focus on the parts you are most interested in and skim the rest. The philosophically inclined may focus on the assumptions, their rationale and the definitions. The mathematically inclined may focus on the definitions, the propositions and their proofs. The scientist on the assumptions, the definitions and their justifications, and the propositions.

\section{Style objective}

The writing style should have the following goals:
\begin{description}
  \item[Skip math] one should be able to skip the mathematical definitions/propositions and still get a coherent narrative.
  \item[Well-defined physical objects] each mathematical objects need to be physically well-defined. It must shown to exist and be unique.
\end{description}
Non-goals:
\begin{description}
  \item[Can't skip physics in justifications] justifications of mathematical definitions will inherently be rooted in the preceding physics discussion.
\end{description}
Notation:
\begin{description}
  \item[state variable and space] state space (cont/discr), state variable (cont/discr), possibility (cont/discr), range of possibility (cont/discr), set of possibilities (cont/discr)
\end{description}

\section{On studying a physical system}

% Status: Conceptual work done. Text ready for review. Incorporated Dave and Isaac feedback

We start by fixing a \emph{physical system}, meaning something we can interact with and perform measurements on (e.g. a planet, a fluid, ...). We call \emph{environment} everything else. We set what particular aspect we want to study (e.g. the motion around a star, the flow in a pipe, ...). We call \emph{state} a physically distinguishable configuration of the aspect under study at a particular time (e.g. position/momentum of center of mass, velocity field, ...). Since the state does not, in general, exhaust the description of the system, a part remains \emph{unstated}, and as such we'll call it, for lack of a better word (e.g. the chemical composition, the motion of each of its molecules, ...). Note that the environment plays an essential role here as it's what allows us to define two states as physically distinguishable: we can find an external process (i.e. part of the environment) whose outcome changes depending on the different state. As such processes are what we can use to perform measurements, we consider the experimental apparatus (and us performing measurements) independent of the system, part of the environment.\footnote{This is true even in the case of general relativity, where we can imagine multiple researchers on small spaceships collecting data without greatly influencing the motion of stars and planets. The case where the physical system is the whole universe and there is no environment presents practical problems  and conceptual challenges that the current physical theories do not seem to be equipped to address, and therefore will be absent from our discussion. For example, what physical device can we use to store and process the state of the whole universe to make predictions and compare? How do we define physically distinguishable? Do the physical laws determine which measurements we are going to make and does that limit what is actually distinguishable?}
 
In this context, we define a process \emph{deterministic} if the state at a given time uniquely identifies states at future times, and \emph{reversible} if it identifies states at past times. While this is a common enough definition, we need to be clear how this applies to the unstated part. That is: the evolution of the unstated part is \emph{always} non-deterministic (and non-reversible) in the sense that the state of the system does not determine its evolution. For example, suppose we define the state as position and momentum of the center of mass of a cannonball. Suppose that the process is deterministic on that state. What does it tell us about the cannonball temperature, or about the motion of each of its atoms? Nothing. In this sense, the unstated part is non-deterministic and non-reversible. Could we extend the state and the laws of evolution to account for temperature? Yes, but that would be a different process defined on a different state. Does it mean the unstated part is always evolving chaotically? Not at all: the temperature may remain constant throughout the motion of the cannonball. Yet we wouldn't know, since we are not studying it: the motion is deterministic and/or reversible only as far as the state is concerned. With this in mind, we introduce the following:

\begin{assump}[Determinism and reversibility]\label{detrevass}
The state of the physical system under study undergoes deterministic and reversible evolution.
\end{assump}

\begin{rationale}
As it is an assumption, we first need to discuss when it is valid. More specifically, we need to understand that the non-deterministic/non-reversible evolution of the unstated part plays as much as a fundamental role as the deterministic/reversible evolution of the state. In fact: the non-deterministic part contributes in determining what states are available to the system.

Suppose we study the motion of a cannonball, its state under gravitational and inertial forces will be properly described by the position and momentum of the center of mass. While light and air molecules scatter off its surface randomly, its trajectory is not greatly affect as it is a massive rigid body. Suppose we study the motion of a small particle, small enough that the random scattering does influence the trajectory and it undergoes Brownian motion: its state will be a probability distribution for position and momentum of the center of mass. Gravitational and inertial forces have not changed, yet the set of states have. In other words: the set of states must also be closed under non-deterministic evolution. If the Brownian motion is not negligible, we do not end in a well defined position/momentum pair, even if we start from one. %Reworded this for Isaac

A similar more drastic effect: consider a book and its motion under gravitation and inertial forces, its state being the position and momentum of the center of mass. As we increase the temperature of the air around the book, its motion remains unaffected until, at some point, the book burns. Clearly, the non-deterministic evolution has brought one of the states outside the set, to the point that the system is no longer recognizable.

As we have hinted, sometimes the state is identified by a distribution (either statistical or actual). Even in this case, the state can be deterministic and reversible. That is: given the distribution at one time we can determine the distribution at future times. The shape and the parameters of the distribution can be deterministic, even if the trajectories of the parts are not (those fall within the unstated part). In fact: we \emph{cannot} assume trajectories and states are always defined at all for the unstated part, as this includes also the unknown unknowns. Will return to this aspect when discussing quantum systems.

It should be also clear that what constitute state and unstated part does not depend only on the system under study, but also on the processes we are considering. In some circumstances, the chemical composition of a fluid may be relevant, in others it may not. The choices of environment, state and unstated part are not independent from each other. By choosing a particular set of states, we are not only saying that the state evolution is well approximated by a deterministic/reversible map from initial to final state, but we are also saying that the non-deterministic/non-reversible evolution of the unstated part does not change the nature of the system, and processes that do not satisfy these conditions are not under consideration.

As with all assumptions, we should also ask whether it is necessary. That is: could we define a set of physically distinguishable states and yet have no deterministic and reversible processes defined on them? The claim is that this assumption is indeed needed, as without it we cannot properly define states or write useful physics laws. We can provide different arguments that point in the same direction.

First, to be able to identify the system, we must be able to tell it apart from anything else. Intuitively, we can distinguish between two chairs because we can move the first to another room and sit on it without having touched the second. We can manipulate the state of the first system without affecting the second, and vice-versa. So, to identify a system it has to be sufficiently isolated from everything else. This means that the system future and past states are with good approximation determined only by its own state: the state undergoes deterministic and reversible evolution.

Second, the aim of physics is to write laws that can be used to make predictions that can be validated experimentally. If I drop an anvil from a tower, it will accelerate at $9.81$~m/s$^2$; if I want the anvil to reach the ground at $x$~m/s I have to drop it from $y$~m. To the extent that we want to make predictions in time, we need to have a one-to-one correspondence between initial and final states.

Third, operationally we must reliably prepare and measure states. That is, we need a process for which the input settings of our preparing device determine the outgoing state of the system; and a process for which the incoming state of the system can be reconstructed by the output of the measuring device. That is, our system must participate in a deterministic and reversible process with the preparing and measuring device. Without it we wouldn't be able to calibrate our experimental apparatus.

Fourth, to be able to ascribe a property to a system we need to claim that, at least for a finite interval of time, the system either held or did not hold such property. That is: there is a deterministic and reversible process for that finite period of time for such property.

This link between state definition and deterministic processes should not be too surprising as the state, in the context of thermodynamics and systems theory, is often defined as \emph{the set of variables needed to determine the future evolution of the system}. As we saw before, this applies also to statistical processes: the distribution (the ensemble) as a whole can be indeed calculated, measured and prepared. We can also describe the evolution of each element of the distribution provided that: we have a way to isolate it and study it under deterministic and reversible motion (so that we can define microstates); the non-deterministic motion does not alter the system (the set of microstates is preserved by the evolution).

As with all assumptions, we should stress that it's an idealization: it can never be completely achieved in practice. A system can be prepared or measured up to a certain level of precision. Perfect determinism and isolation of a system is impossible both practically (e.g. black-body radiation, gravity, ...) and conceptually (e.g. if the system is perfectly isolated, we cannot interact with it: how can it be physically distinguished?). It's a simplifying assumption that can only be taken if the environment and the internal dynamics of the system interact in such a way that they little affect and are little affected by the aspect we are studying. As we saw before, for example, assuming that the state consists of the position and momentum of the center of mass requires assuming that the Brownian motion of the body is negligible.

Yet, this is a fundamental assumption in the sense that it is needed. If a particular set of states does not satisfy deterministic and reversible evolution under certain conditions, what we do is to keep at it until we find a set that does. That is: we work to restore the assumption. Finding new sets of states with new laws of evolution is, in fact, what leads to new physics. We will therefore call \emph{fundamental model of physics} the triad of state, unstated part and environment, with the assumption that the state may undergo deterministic and reversible evolution, and the unstated part undergoes non-deterministic non-reversible evolution that does not alter the set of states.
\end{rationale}

\section{States and state space}

% Status: concepts still need to be finished. Started to work on linking topology to physically distinguishable states.

%Open questions: smoothness (obviously required to define densities later, is it required in general? What does it means that physical processes/measurements don't give a smooth topology?)

%Things to add: discrete sets evolved continuous = identity map. Subdivide finite interval, same permutation at each time step, exclude odd perms, third-odd perms, ...

We now proceed to characterize states and their being physically distinguishable in more precise terms so that we can capture their description mathematically. What we'll see is that the set of all physical processes that can be used to gain information about the system, that can be used to perform a measurement, induce a topology on the set of states that is Hausdorff and second countable. That is: physical distinguishability  is mathematically captured by topological distinguishability. Deterministic and reversible evolution will then preserve the topology, and they will be mathematically captured by self-homeomorphisms.

We'll focus on state spaces that can be described by a set of independent state variables, either discrete or continuous, and see what can be said in general on the evolution of state variables.

\subsection{States and topology}

As the term measurement has become a particularly loaded, let's first characterize what we mean by physical distinguishability in our context.

Consider the motion of a cannonball under inertial and gravitational forces. Light will scatter off of it; as it lands the ground will be deformed and the impact will make the temperature rise. Those external physical processes, which happen no matter what we do, can be used to distinguish the motion of the cannonball as their outcomes are correlated.\footnote{Note that by requiring a correlation we are implying that measurements are repeatable. That is, for similarly prepared states we get similar outcomes.} Therefore we can learn the position by looking at the reflected light, and learn the final kinetic energy by looking at the deformation of the ground. In principle, any external process that has a correlation with the states under study can be used to perform a measurement, and any measurement is based on such a process. That is: for us a measurement is simply a physical process that we can use to distinguish states. There is no particular role played by the observer, except maybe setting up such a process and forcing the system under study to interact with it.

This external process may be quite complicated: when a particle enters a calorimeter, a shower of particles is produced, photons are captured and are directed to photomultipliers, a current is read out, the current is then digitized, and so on. Some processes interfere with the system, they affect its dynamics, and other are destructive, the system no longer can be described by the original set of states. A tracking chamber is an example of the first (the magnetic field curves the motion of a charged particle); burning a substance to determine its caloric content is an example of the latter. In these cases, special care is needed to make sure to link the outcomes with the original state. But in the end, the conceptual description remains the same: each possible outcome of the process will be associated with a set of states consistent with that outcome (e.g. if the cannonball deformed the ground by this amount than the final state its kinetic energy at impact was within this range).

Each physical process, then, can be thought as having a set of potential outcomes. To each potential outcome we can associate a set of states that are consistent with it. For example, if the electron follows a certain path, then its state is among the ones that has spin $+1/2$. However precise our measurements are, each outcome is expressed by a finite set of digits, therefore the set of outcomes is countable. Note that different outcomes for the same process can overlap. For example, $4.12 \pm 0.05$ cm and $4.13 \pm 0.05$ cm are both legitimate possible outputs of the same measurement device. But since all states must be distinguishable, given two arbitrary states there must be a process precise enough to tell them apart. That is: the potential outcomes associated with the two states do not overlap. For example, $4.12 \pm 0.0005$ cm and $4.13 \pm 0.0005$ cm do not overlap anymore.

We can also conceptually combine two different processes into a single one. That is: having a way to measure quantity $x$ and a way to measure quantity $y$ gives us a way to measure the combination $(x,y)$. Formally, the outcomes of the combined process will be the intersections of each pair of outcomes of the original processes.

We can also coarsen a single process. That is: having a way to measure $(x,y)$ gives us a way to measure $x$ alone (or any $f(x,y)$). Formally, the outcomes of the coarsened process are given by performing the union of some outcomes of the original process.

This model maps very naturally to a topological space. The states are elements of a set and the physical outcome provides a topology on that set. The fact that two elements of the set can be distinguished requires the space to be Hausdorff. The fact that the potential outcome of a process are countable requires the space to be second countable.

\begin{defn}\label{statedef}
The state space $\mathcal{S}$ of a physical system is a Hausdorff and second countable topological space.
\end{defn}

\begin{justification}
TODO: all possible states form a set. All possible set of states associated with the outcome of the process form a topology. Verify requirements on entire set, empty set, intersections and unions. Distinguishability of two elements in terms of process outcome leads to Hausdorff. Processes have countable possible outcome means base for topology must be countable, or we would have elements that are not distinguishable, therefore second countability.
\end{justification}

With our state space defined, deterministic and reversible evolution corresponds to a bijective map between initial and final states. Moreover, a physical process that can distinguish initial states is also a physical process that can distinguish final states. That is: the topology is mapped and preserved by the deterministic and reversible evolution (i.e. initial and final states are equally distinguishable). The evolution is then a self-homeomorphism on the state space.

\begin{defn}\label{detrevmap}
A deterministic and reversible evolution map is a self-homeomorphism on the state space $f:\mathcal{S} \rightarrow \mathcal{S}$.
\end{defn}

\begin{justification}
We claim that $f$ exists. The system is deterministic: given an initial state $\mathcal{s} \in \mathcal{S}$ there exists a well defined final state $f(\mathcal{s})=\hat{\mathcal{s}} \in \mathcal{S}$.

We claim that $f$ is a bijection. The system is reversible: there exists a map $g:\mathcal{S} \rightarrow \mathcal{S}$ that returns the initial state given the final state. $g \circ f = f \circ g = id$ as mapping forward and then backward or backwards and then forward must return the original element. $f$ admits $g$ as an inverse. $f$ is a bijection

We claim $f$ is an open map. Let $U \subseteq \mathcal{S}$ represent an outcome of a process. $U$ is an open set in the state space topology by definition. Given the deterministic and reversible evolution $f$, an outcome on the initial states can be used to distinguish final states. The same outcome associated with $U$ as initial states will also be associated with $f(U)$ as final states. $f(U)$ is associated with an outcome. $f(U)$ is an open set in the state space topology by definition.
\end{justification}

%TODO: do continuous function form a manifold Hausdorf and second countable?

\subsection{Labeling states}

To identify and name states we will use a set of quantities, typically numbers. For example, the orbital of an electron in a hydrogen item is identified by the quantum numbers $n$, $l$, $m$ and $s$. We call each of these quantities \emph{state variable} and \emph{state vector} a set of state variables that can identify one and only one state. We avoid the term coordinates, even though that's what they mathematically are, as it will create confusion with space coordinates. We also avoid the term observable or measurable, as not all state variables map directly to the outcome of an experiment (e.g. conjugate momentum as it is not gauge invariant). It's clear, though, that even in those cases there must exist, at least in principle, a way to distinguish between the possible values of the state variable (e.g once we fix the vector potential, conjugate momentum can be measured via the kinetic momentum). Therefore we can assume that all state variables are in the end physically distinguishable, yet we do not assume all state variables have equal standing in physical meaning. Conversely, some measurable quantities are not directly state variables: they may not be defined, as states are, at constant time (e.g. velocity) or may depend on the process at hand (e.g. acceleration). Yet, once a process is fully defined (e.g. a Hamiltonian is given), we may have a relationship between those observables and state variables. The purpose of state variables is just to label states, which are physically distinguishable and defined at a particular time. Therefore the state variable will be physically distinguishable and defined at constant time. But there is nothing telling us that they must be directly physically meaningful or that their relationship to measurable quantities is the same if we change physical processes.

TODO: maybe best to define state vector directly, and state variables as part of state vector? Define discrete state variable when co-domain is integers and define continuous state variable when co-domain is real? Can we prove a state vector always exists, of real variables? For discrete topology, we should always be able to do it. In general, state vector defines charts, so we should not have a single chart for the whole state space. What does it mean physically? Does it matter?

\begin{defn}\label{state_variable}
Let $\mathbbm{I}$ be a set. We call $q : \mathcal{S} \rightarrow \mathbbm{I}$ a \emph{state variable} and a \emph{possible value} (or \emph{possibility}) of $q$ an element $i \in \mathbbm{I}$. Also $\mathcal{s}_1 \neq \mathcal{s}_2 \Leftrightarrow \exists q : \mathcal{S} \rightarrow \mathbbm{I} | q(\mathcal{s}_1)\neq q(\mathcal{s}_2) \forall \mathcal{s}_1, \mathcal{s}_2 \in \mathcal{S}$.
\end{defn}

\begin{justification}
Consider a physical process that can distinguish between different initial conditions. Let $\mathbbm{I}$ be the set of all possible outcomes. To each possible state $\mathcal{s}$ will correspond one and only one outcome. Let $q : \mathcal{S} \rightarrow \mathbbm{I}$ be the map between state and outcome. Such object is physically well defined.

Let $\mathcal{s}_1, \mathcal{s}_2 \in \mathcal{S}$. Suppose $\mathcal{s}_1 \neq \mathcal{s}_2$. $\mathcal{s}_1$ and $\mathcal{s}_2$ are physically distinguishable. There exists a physical process with different outcomes. Let $q : \mathcal{S} \rightarrow \mathbbm{I}$ be the state variable associated to that process. Then $q(\mathcal{s}_1)\neq q(\mathcal{s}_2)$. Now suppose $\nexists q : \mathcal{S} \rightarrow \mathbbm{I} | q(\mathcal{s}_1) \neq q(\mathcal{s}_2)$. Then no process can distinguish between the two, they are physically indistinguishable therefore $\mathcal{s}_1 = \mathcal{s}_2$.
\end{justification}

TODO: Still not clear what's the best way to introduce things. We do want to show that state variable are maps from states to "labels". We want to call discrete variables those that map to integer and continuous variables those that map to real numbers. Then we want to concentrate on state space that can be described by a finite number of either discrete variables or continuous variables. State vector may not be valid in general: e.g. we can't assume we have a single chart for the whole space. 

\begin{prop}\label{state_vector}
TODO: State vector always exists
\end{prop}

To study time evolution, we need to describe how state variables change under continuous and reversible evolution. There are two ways to do it, and we'll need both. The first approach is to \emph{evolve} the state variables from the initial value $q^i(\mathcal{s})$ to the final value $q^i(\hat{\mathcal{s}})$. The result is a trajectory $q^i(t)$ which is especially useful when state variables correspond to physically meaningful quantities. For example, we track how the temperature or the pressure of an ideal gas changes. Evolved state variables, however, make it hard to find relationships that are invariant and common to all deterministic and reversible processes.

The second approach is to transport the state variables. The idea is to keep the connection to the initial state by labeling the future state, instead by the future values of $q^i$, by the original one. So we introduce a new set of variables for which $\hat{q}^i(\hat{\mathcal{s}})=q^i(\mathcal{s})$, which we can always do as the evolution is deterministic and reversible. The level sets of $\hat{q}^i$ are the evolved level sets of $q^i$, which allows us to study how groups of states evolve in time. Moreover, once we identify an invariant for $q^i$ this is automatically satisfied by $\hat{q}^i$. Given that the value of the transported state variables does not change during evolution, and that it is unique for each initial state, transported state variable provide a way to label the trajectories themselves. While the numeric value of the transported state variables remains the same, the state variable (and its physical meaning) changes. For example, pressure and temperature will be combined depending on the particular process.

Evolved state variables are useful to write equations of motions, study how physical quantities change, form a physical picture of what happens. Transported state variables are useful to write invariants, study state space trajectories, from a geometric picture for the state space. The two, though, are tightly linked. In fact:
\begin{align*}
\Delta q^i &= q^i(\hat{\mathcal{s}}) - q^i(\mathcal{s}) \\
\hat{q}^i(\hat{\mathcal{s}}) &= q^i(\hat{\mathcal{s}}) - \Delta q^i
\end{align*}
Intuitively, the transported variables change in the oppose way as the evolution, so that the value of the variables remain the same.\footnote{One should not confuse evolved/transported state variable with active/passive transformations or with Schroedinger/Heisenberg pictures. In those cases, the choice is whether to change the state or the coordinates/observables. In our case, the state is always changing. The choice is whether to change the state variables or their value.}

\begin{defn}\label{evolved_transported_variable}
TODO: Define evolved and transported variables in terms of pushback/pushforward
\end{defn}

\begin{justification}
TODO: prove existence
\end{justification}

When assembling multiple state variables into a state vector, it is important to understand how they relate to each other. Consider the orbital of an electron in a hydrogen item, which is identified by the quantum numbers $n$, $l$, $m$ and $s$. For each combination of $n$, $l$ and $m$, the spin $s$ can have two values. The choice of spin is independent from the rest. The choice of $l$, though, depends on the choice of $n$: for $n=1$ only $l=s$ is available; for $n=2$ we can choose $l=s$ or $l=p$. The choices are not independent. That is: two or more variables are independent if there always exists a state for any possible combination, if the total number of states is the cartesian product of the possibilities of each variable.

For discrete state variables, the total number of states is the product of the possibilities of each independent state variable. That is: if $\Delta q^1$ and $\Delta q^2$ are range of possibilities for two independent variables, the total number of possibilities $\#(\Delta q^1) \#(\Delta q^2)$. These relationships are maintained during evolution by the transported variables. Since the value remains unchanged, the range of possibilities for each value remain the same: $\#(\Delta \hat{q}^i) = \#(\Delta q^i)$. The variables remain independent as well, all possible combination remain, and we have: $\#(\Delta \hat{q}^1) \#(\Delta \hat{q}^2) = \#(\Delta q^1) \#(\Delta q^2)$. These relationship will later be extended in the continuous case to independent degrees of freedom for a Hamiltonian system, and will justify the symplectic form. Though this can't be done in general and requires further assumptions.

In the case of continuous state, if each state can be identified by n independent state variables then, at least locally, we can chart the state space on $\mathcal{R}$: the state space is a manifold. Studying in general how ranges of possibilities evolve, though, becomes problematic. Time evolution will map points, while range of continuous possibilities will be length. We would need a metric, invariant during the evolution, telling us how ranges of possibilities map from one variable to another, possibly describing different physical quantities. We will not study this case, but we do note that, if such a metric exist, it depends on the system and on the choice of state variables.

\begin{defn}\label{independent_state_variables}
Let $q_1 : \mathcal{S} \rightarrow \mathbbm{I}_1$ and $q_2 : \mathcal{S} \rightarrow \mathbbm{I}_2$ be two state variables. They are said \emph{independent} if $q_1^{-1}(i_1)\cap q_2^{-1}(i_2) \neq 0 \forall i_1 \in \mathbbm{I}_1, i_2 \in \mathbbm{I}_2$.
\end{defn}

\begin{justification}
If $q_1$ are $q_2$ independent state variables we can modify a state such that only one variable is affected. TODO finish
\end{justification}

\begin{prop}\label{discrete_state_space}
The space state $\mathcal{S}$ for a system fully identified by a set of $n$ independent discrete state variables $q^i : \mathcal{S} \rightarrow \mathbbm{I}^i$ is isomorphic to $\prod\limits_{i=1}^n\mathbbm{I}^i$. Let $I^i \subseteq \mathbbm{I}^i$ be finite sets of possibilities for each variable. $\#(\bigcap\limits_{i=1}^{n}{q^i}^{-1}(I^i))=\prod\limits_{i=1}^{n}\#(I^i)$.
\end{prop}

\begin{proof}
TODO
\end{proof}

TODO: in continuous, not necessarily the Cartesian product. But we can always create a chart with state variables. State space is a manifold of dimension n. In principle, we could say more. For example, a deterministic/reversible map would not map a set to its subset or superset. We could define a metric, this would give us an ergotic system. Not going to do that.

\begin{defn}\label{continuous_state_space}
The space state $\mathcal{Q}$ for a system fully identified by a set of $n$ independent continuous state variables is a manifold of dimension $n$.
\end{defn}

\begin{justification}
TODO: set of state variables locally define a chart, isomorphic to $\mathbbm{R}$.
\end{justification}

We have seen that physical systems undergoing deterministic and reversible evolution are properly described by a dynamical system. Systems described by a discrete set of states have nice properties for counting possibilities of independent variables, which are maintained during the evolution. System described by a continuous set of states, instead, do not posses in general such properties.

\section{Composite systems and reducibility}

%Status: conceptual work is done, need to write down

Given that scientific reductionism (i.e. the idea of reducing physical systems and interactions to the sum of their constituent parts in order to make them easier to study) is at the heart of fundamental physics, we should explore how to characterize a system in terms of its components. In general, this is quite a complicated thing to do, that requires intimate knowledge of the system at hand. So we simplify our problem and study an idealized case: one where the components are homogeneous and infinitesimal. What we'll find is that under the additional assumption that the system is reducible (i.e. its state is equivalent to the state of the parts) and that each part undergoes deterministic and reversible evolution, the motion is suitably described by the standard framework of classical Hamiltonian particle mechanics.

\subsection{Homogeneous decomposable systems}
The notion that a system is decomposable means the states is equipped with a rule of composition that allows to write $\mathcal{c}=\mathcal{c}_1+\mathcal{c}_2$: the composite system is the sum of its part. For example, the state of a ball is equal to the state of its left and right parts.

The notion that the system is homogeneous means that the states of the composite and of the parts are not unrelated: they are made of the same material.\footnote{Whether a system is homogeneous depends on context (e.g. air can be thought as homogeneous if the mixture of gases does not change in space or in time due to phase transitions or chemical processes) and one must check that such property is maintained by by time evolution.} In fact, the state space $\mathcal{C}$ of all systems composed of such material will contain the state of system and all its part (i.e $\mathcal{c}, \mathcal{c}_1, \mathcal{c}_2 \in \mathcal{C}$). Since combining any two systems made of a homogeneous material will always give us a system made of the same homogeneous material (e.g. combining elements made of water gives us another element made of water), the state space $\mathcal{C}$ is closed under composition.

As we study the system under evolution, we'll also want to study state changes. For example, if a stable mixture of gas expands, we'll want to know the difference between the initial and final distributions. That is: $\Delta\mathcal{c}=\hat{\mathcal{c}}-\mathcal{c}$. Note that such state difference may not describe a physical state: it may remove some material from one location to add it somewhere else. Yet, these state changes are still physically distinguishable objects that provide a configuration for the same homogeneous material, therefore we extend the state space $\mathcal{C}$ to include state differences. All combined, this gives us the structure of an abelian group.

\begin{defn}\label{reducible_state_space}
The state space $\mathcal{C}$ for decomposable systems made of a homogeneous material is an additive abelian (i.e. commutative) group.
\end{defn}

\begin{justification}
We claim $\mathcal{C}$ is an additive monoid. There exist a law of composition $+ : \mathcal{C} \times \mathcal{C} \rightarrow \mathcal{C}$ that takes two states and returns one that is the physical composition of the two. The domain and codomain match because the material is homogeneous. The law is commutative $\mathcal{c}_1 +\mathcal{c}_2 = \mathcal{c}_2+\mathcal{c}_1$ and associative $(\mathcal{c}_1 + \mathcal{c}_2) + \mathcal{c}_3 = \mathcal{c}_1 + (\mathcal{c}_2 + \mathcal{c}_3)$, as it does not matter in what order we physically compose the parts. There exist a unique zero element $\mathcal{c} + 0 = \mathcal{c}$ and it represent the physically empty state (i.e. no amount of material).

We claim $\mathcal{C}$ is an additive group. As we want to describe changes during the evolution (i.e. $\hat{\mathcal{c}} = \mathcal{c} +\Delta \mathcal{c}$) we introduce an inverse $- : \mathcal{C} \rightarrow \mathcal{C}$ such that $\mathcal{c} + ( - \mathcal{c}) = 0 \forall \mathcal{c} \in \mathcal{C}$. Such inverse may introduce objects that do not properly represent a physical state, but a state change. A change of a physically distinguishable object is itself physically distinguishable therefore $\mathcal{C}$ is still a Hausdorff and second countable topological manifold as per \ref{statedef}. We'll still call $\mathcal{C}$ state space committing an abuse of terminology.
\end{justification}

\subsection{Classical homogeneous systems}
We also want to be able to express the state of the composite system in terms of the state of the parts. For example, given the state of a fluid we'll want to know how its parts are distributed. This in general will depend on how much the state of the composite system "knows" about its parts. For example, the position and orientation of an ideal rigid body is enough to define where all its constituents are, while the volume/pressure/temperature of an ideal gas is not enough to determine the position and momentum of all its molecules. Therefore we need to characterize the system further.

We will call a classical system one that is infinitesimally reducible. That is: it is made of arbitrarily small parts, which we call particles, and the state of the whole system is equivalent to the sum of the states of the parts. Given the state space $\mathcal{S}$ for the infinitesimal parts, a composite state $\mathcal{c}$ will tell us the amount of material for each possible particle state. That is, each state is fully identified by a function $a: \mathcal{S} \to \mathbbm{R}$ that returns the amount of material in the composite state $\mathcal{c}$ that is prepared in state $\mathcal{s}$. For example, given a certain configuration of gas, we can tell the amount that is at a specific point in space with a specific value of momentum. The notion that the parts are infinitesimal requires the co-domain of $a$ to be continuous, as opposed to discrete. The state space $\mathcal{S}$, instead, can be finite. For example, for a system composed of different tanks connected by pipes, the state of the composite system could be the overall distribution of water among the tanks. The amount in each tank is continuous (as we assume the water to be infinitesimally divisible) yet there are only a finite number of tanks the water can be placed into.

As we combine the state of finite parts, we sum the distributions over particle states $a(\mathcal{s})=a_1(\mathcal{s})+a_2(\mathcal{s})$: the amount of material in the composite state is the sum of the amount of material of the parts. This gives us the structure of a real vector space.

\begin{defn}\label{classical_vector space}
The state space $\mathcal{C}$ for a homogeneous classical (i.e. infinitesimally reducible) material is a vector space over $\mathbbm{R}$. It admits a basis isomorphic to the state space $\mathcal{S}$ of its particles (i.e. infinitesimal parts).
\end{defn}

\begin{justification}
The state space is an abelian group as the material is homogeneous and decomposable and \ref{reducible_state_space}.

We now consider the set of transformations $T$ that increase or decrease the amount of material in the system by a constant. We claim $T$ is a field\footnote{Here field is intended in the abstract algebraic sense (a nonzero commutative division ring) which has no relationship to the field in the physics sense (a physical quantity with a value for each point in space).} isomorphic to $\mathbbm{R}$. Consider $\tau: \mathbbm{R} \rightarrow T$ the mapping between a number and the transformation that increases or decreases the amount of material by that number. This transformation exists: the system is infinitesimally decomposable and the amount can be changed continuously. Define on $T$ an addition $+: T \times T \rightarrow T$ and a multiplication $*: T \times T \rightarrow T$ such that $\tau(a) + \tau(b) = \tau(a+b)$ and $\tau(a) * \tau(b) = \tau(a*b)$, $a,b \in \mathbbm{R}$, so that the sum and product of the transformation is equal to the sum and product of their respective factors. $\tau$ is an isomorphism between $T$ and $\mathbbm{R}$ as fields.

We now claim that the state space $\mathcal{C}$ is a vector space over $\mathbbm{R}$. The abelian group $\mathcal{C}$ can be extended with the operations defined by $T$, as each element $\tau \in T$ is a map $\tau : \mathcal{C} \rightarrow \mathcal{C}$. The map has the following properties: $(\tau_1 + \tau_2) \mathcal{c} = \tau_1 \mathcal{c} + \tau_2 \mathcal{c}$, increasing the amount of material by the sum of two constant is the same as combining the separate increases, and $\tau (\mathcal{c}_1 + \mathcal{c}_2) = \tau \mathcal{c}_1 + a \mathcal{c}_2$, increasing the amount of the total system is the same as the combination of the increased parts. $\mathcal{C}$ is a module over $T$, which is a field and isomorphic to $\mathbbm{R}$. $\mathcal{C}$ is (isomorphic to) a real vector space.

Consider now the injection map $\mathcal{e}: \mathcal{S} \hookrightarrow \mathcal{C}$ that for each particle state returns a composite state constituted only by a predetermined unit amount of material in such state. The image of such injection is a basis for $\mathcal{C}$. We first claim that all elements are linearly independent. Consider $\sum\limits_{\mathcal{s} \in \mathcal{S}} \tau(a(\mathcal{s})) \mathcal{e}(\mathcal{s})$, $a: \mathcal{S} \to \mathbbm{R}$. This corresponds to a physical state with $a(\mathcal{s})$ material in each state $\mathcal{s}$. This is equal to the empty state if and only if $a(\mathcal{s})=0 \forall \mathcal{s} \in \mathcal{S}$. We now claim they span. Consider $\mathcal{c} + \sum\limits_{\mathcal{s} \in \mathcal{S}} \tau(a(\mathcal{s})) \mathcal{e}(\mathcal{s})$. This can always be made equal to the empty state by setting $a(\mathcal{s})$ to be the opposite of the amount of material of $\mathcal{c}$ found in state $\mathcal{s}$. Both arguments hold even if $\mathcal{S}$ is an infinite or uncountable set, as they hold for each finite subset of $\mathcal{S}$ independently and therefore $\mathcal{C}=\prod\limits_{\mathcal{s} \in \mathcal{S}} span({\mathcal{e}(\mathcal{s})})$.
\end{justification}

Need to define inner product. Introduce $Q:\mathcal{C} \to \mathbbm{R}$ amount of "stuff" of the system. $Q:\mathcal{S}\times\mathcal{C} \to \mathbbm{R}$ amount of stuff of the composite system in that particular state. We can use those those to construct inner product, and to show that states are bounded, but: norm induced by the inner product is not physical.

\begin{defn}\label{classical_inner_product}
	TODO: there exist an inner product.
\end{defn}

\begin{justification}
	TODO
\end{justification}

Show countable basis. $\mathcal{c}= \sum\limits_{\mathcal{s} \in \mathcal{S}} a(\mathcal{s}) \mathcal{e}(\mathcal{s})=\sum\limits_{q \in \mathbbm{Q}} a(q) \mathcal{e}(q)$

% Continuous state variable: Q is measured at constant t

Uncountable basis does not work. $\mathcal{c}= \int\limits_{q \in \mathbbm{Q}} \rho(q) \mathcal{e}(q)dq$ but $\rho(q')=|\partial _{q}q'|\rho(q)\neq\rho(q)$

TODO: need to understand a valid notation for composite state as infinite sum. The densities is the coefficient, the basis are are states, what is the differential?

Find smallest object that can have an invariant density (inner product). $\mathcal{c}= \int\limits_{T^*\mathbbm{Q}} \rho(q,k) \mathcal{e}(q,k)dq\wedge dk$

\begin{defn}\label{classical_phase_space}
The state space $\mathcal{S}$ for the particles of a homogeneous classical  system is a cotangent bundle $T^*\mathbbm{Q}$.
\end{defn}

\begin{justification}
	TODO
\end{justification}

TODO: generalization of inner product gives us $\int \rho_1(q, p) \rho_2(q, p) dq\wedge wp$. Space of continuous functions is not a complete metric space. Extend to square integrable. Get a Hilbert space. But topology is not a second countable: more than aleph 1. No physical justification, in fact we introduce unphysical functions (i.e. 0 rational, 1 irrational). Discontinuous functions not physical states: they are not distinguishable.

Density allows to count particle states. Example: two uniform distribution for the same amount of particles.

Area in each d.o.f. is number of possibilities. Total number of states is product of number of possibilities for independent d.o.f. Independent d.o.f. are orthogonal. Leads to omega metric. As in discrete case, conserved during evolution.

Also: we can search for omega looking from constraints. Must be linear. Must be skew-symmetric (applied to the same vector twice must be zero). Must be invariant by coord transformation. 

\begin{defn}\label{symplectic_manifold}
	The state space $\mathcal{S}$ for the particles of a homogeneous classical  system is a symplectic manifold with metric $\omega$, the canonical two-form.
\end{defn}

\begin{justification}
	TODO
\end{justification}

\subsection{Classical hamiltonian assumption}

% Det/rev for infinitesimal part
% Inconsistent: 1. X=AxB A'=F(A) B'=F(B) Isolated sub-systems. Does not work for measuring. 2. A'=F(A) B'=F(A,B) Ideal non-interfering measurement: deterministic but not reversible. 3. A'=F(A,B) B'=F(A,B) X det/rev whole, but A not deterministic itself. But if you consider X a sub-system of X and something else, you can make the same argument. Whole universe deterministic, but none of its parts. 4. A=As x Au ; B=Bs x Bu ; As' = F(As) Au=F(X) Bu=F(X) Bs = F(As, Au) ; more realistic picture. Still non-reversible (if As' = F(As), then nothing else can be F(As) and have bijective map).

We further characterize the relationship between the whole and the parts with the following assumption:

\begin{assump}[Classical hamiltonian assumption]\label{classical}
	The evolution of the classical homogeneous system under study is deterministic, reversible and reducible.
\end{assump}

In other words: if we know the state and the evolution for the whole system, we also know the evolution for all its parts; if we know the state and evolution for all the parts, we know the state and the evolution for the whole system. By smaller we do not mean spatial extent, but of smaller amount (e.g. less mass): even at the same spatial location we can imagine that there are infinitely many components (e.g. the mass, as a number, is continuously divisible(.

\begin{rationale}
	This is clearly a \emph{simplifying} assumption, and it is instructive to understand when it breaks down.
	
	The first problem is methodological. As we saw before, we need access to a deterministic and reversible process to be able to study a system and define a state. For two balls, we can imagine to isolate some pieces (small enough to be considered infinitesimal in respect to the ball but big enough to contain enough molecules to be considered homogenous) and then describe the collision between the two by describing what happens at each piece. The classical assumption holds. For an electron and a photon, we cannot take pieces of the electron or the photon, study them in isolation and then describe how each part moves during Compton scattering. The classical assumption does not hold: we do not have suitable physical processes at our disposal.
	
	The second problem is more conceptual. As we saw before, a system cannot be fully isolated from the environment. A certain amount of unstated part needs to remain so that the state of the system can be insulated from the external non-deterministic interaction. In practice, the state of each piece of our material is still given by the average of a collection of a large amount of molecules, their respective motion depending not just on the state of the system but on the external environment.  The classical assumption can therefore hold only if we assume that the description of each pieces is, again, not complete. Assuming it at a fundamental level would mean that the motion of each and every molecule of the system is only influenced by the state of the system, no external factor counts. As such, it would be a contradiction with it being a physical system: one we can study experimentally.
	
	While ultimately flawed, the classical assumption can be considered valid for a great number of macroscopic system, and that is why is very useful. One should be cautioned, though, that nowhere in the assumption the spatial extent of the system is mentioned. We may as well have a macroscopic system where a clear independent state cannot be assigned to each part, and the assumption would not hold.
\end{rationale}

We are now ready to capture the elements of our discussion and their properties through mathematical definitions.


\begin{defn}\label{canonical_transformation}
	A deterministic and reversible process is a diffeomorphism that preserves $\omega$, that is a canonical transformation.
\end{defn}

\begin{justification}
	TODO
\end{justification}

\begin{defn}\label{hamiltonian}
	A continuous deterministic and reversible process admits a potential $H$, and the laws of evolution are of the form (Hamilton equations).
\end{defn}

\begin{justification}
	TODO
\end{justification}



\section{Quantum systems}
%Note on composite systems and distinguishability. When putting together two classical system, just sum distributions. When putting together quantum system we have to clarify: combine to create one quantum system (sum in vector space) or two separate quantum system (symmetrized tensor product). In classical mechanics, 2a + 2b = (a+b) + (a+b). In quantum note |a> x |b> = |a>|b> + |b>|a> but (|a> + |b>) x (|a> + |b>) = |a>|a> + ... That is two quantum states in two different states is not the same as two quantum states spread equally over those two states. In classical they are.)

% Muon example. Consider a muon and its decay into an electron and two neutrinos: it is clear that the three outgoing particles have a state and trajectory of their own, it is clear that the resulting total mass and energy came from the muon. Yet, before the decay, we cannot ascribe an internal independent state and evolution to each part. The state of a muon is not some combination of the state of an electron and two neutrinos. That is: the unstated part is not just microstates.


%TODO: moved from classical
%This intuitive picture is unfortunately not suitable to be generalized to different contexts, so we develop another.

%Consider the following expression:
%\begin{align*}
%\mathcal{c} = \sum\limits_{i \in \mathbbm{I}} a_i %\mathcal{e}(i)
%\end{align*}
%$\mathbbm{I}$ represents all the possible values of a state variable representing the best description of an infinitesimal part that a composite system can give. $\mathcal{e}(i)$ represents a composite system made of a unit amount of particles, all prepared in the state determined by $i$. $a_i$ represents a transformation that changes the whole state, without affecting the value $i$ for any infinitesimal part. For a classical system, $i$ represents the full information about the infinitesimal part, and therefore includes the value of all state variables (i.e. the state vector); the only transformation left is increasing/decreasing the amount of particles by a scalar multiple. For a different system, where $i$ does not represent the full state of the parts, $\mathcal{e}(i)$ will be an ensemble and $a_i$ also represents an internal transformation of such ensemble. This is intuitively how we decouple the composite state into the part $i$ that describes the infinitesimal parts, and the part $a_i$ that describes how the parts are composed. Giving a full description of the state space of a decomposable system means fully characterizing these components: the state variable $\mathbbm{I}$ and the set of transformations $A$. In the classical case $\mathcal{e}(i)$ form a basis of a real vector space, where $i$ is the state vector for an infinitesimal part, and $a_i$ a real valued coefficient that represents the amount of particles in each state, which describes how the parts are combined.


\begin{thebibliography}{0}

\bibitem{Shannon} Shannon, C. E., ``A mathematical theory of communication'', The Bell System Technical Journal, Vol. 27, pp. 379--423, 623--656, (1948).
\bibitem{Jaynes} Jaynes, E. T., ``Information theory and statistical mechanics'', Statistical Physics 3, pp. 181--218, (1963).
\bibitem{classical_dynamics} J. V. Jos\'{e}, E. J. Saletan, ``Classical Dynamics'', Cambridge University Press, (1998).
\bibitem{Gromov} Gromov, M. L., ``Pseudo holomorphic curves in symplectic manifolds'', Inventiones Mathematicae 82, pp. 307--347, (1985).
\bibitem{deGosson} de Gosson, M. A., ``The symplectic camel and the uncertainty principle: the tip of an iceberg?'', Foundations of Physics 39, pp. 194--214, (2009).
\bibitem{Stewart} Stewart, I., ``The symplectic camel'', Nature 329, pp. 17--18, (1987).
\bibitem{Lanczos} Lanczos, C., ``The variational principles of mechanics'', University of Toronto Press, (1949).
\bibitem{Synge} Synge, J. L., ``Classical dynamics'', Encyclopedia of Physics Vol 3/1, Springer (1960).
\bibitem{Struckmeier} Struckmeier, J., ``Hamiltonian dynamics on the symplectic extended phase space for autonomous and non-autonomous systems'', J. Phys. A: Math. Gen. 38, pp. 1257--1278, (2005).

\end{thebibliography}

\end{document}
