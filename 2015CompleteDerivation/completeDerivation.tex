\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}

\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{dutchcal}

\newtheorem{assump}{Assumption}
\renewcommand*{\theassump}{\Roman{assump}}
\newtheorem{prop}{Proposition}[section]

\theoremstyle{definition}
\newtheorem{defn}[prop]{Definition}

\newenvironment{rationale}{\emph{Rationale}.}{\qed}
\newenvironment{justification}{\emph{Justification}.}{\qed}
\renewenvironment{proof}{\emph{Proof}.}{\qed}

\begin{document}

\title{From physical principles to ???}
\author{Gabriele Carcassi}
\affiliation{University of Michigan, Ann Arbor, MI 48109}
\email{carcassi@umich.edu}
\date{March 15, 2015}

\begin{abstract}
Deterministic/reversible assumption. Describe fundamental model of physics. System is always interacting with environment, both deterministically and non-deterministically. Deterministic means only depends on the state of the system. Set of states needs to be invariant under non-deterministic interaction: the non-deterministic interaction affects what are the states of the system. Deterministic/reversible evolution is necessary to even be able to talk about a system (trivial deterministic process).

Define: set of states, deterministic evolution as permutation, labels/state variables, cardinality of state variable, state id function (=1 for a particular state). Can we do for continuous variables as well? Can we find divergence free evolution for det/rev and independent variables?

Infinite reducibility assumption. Decomposable systems. Rule of composition. Inverse and null state to study changes. Set of transformations to increase and decrease the quantity in each state. Define magnitude of the system (probably needed to guarantee things converge in math?).

Define: set of states as a vector space, inner product.

Continuous quantities. Need for densities to be a function of the state (not state variable). Defined on infinitesimal interval. Define: degree of freedom, state cardinality, metric (T*Q).
\end{abstract}
\maketitle

\section{Introduction}

While some branches of fundamental physics (e.g. Newtonian mechanics and special relativity) are founded on physical laws or principles others (e.g. Lagrangian/Hamiltonian/Quantum mechanics) start by postulating mathematical frameworks or relationships. The latter approach presents a significant drawback: the mathematical structure is not enough to define the physical system it describes. We are left asking: under what physical conditions a particular system is described by Lagrangian (or Quantum) mechanics? Naturally one \emph{can} define a Lagrangian system (or a Quantum system) as one that obeys those rules, but that begs the question. Another issue is that each element of the mathematical structure may or may not correspond to some physical concept. Suppose the earning (or losses) of two companies are governed by Hamiltonian's equations, which variable is on the cotangent space of the other? In other words: if we want to fully appreciate what the fundamental theories of physics are describing, what each mathematical concept corresponds to in the physical world, we need to start by characterizing the physical system we are describing in a precise enough way that it can be encoded in mathematical language. Naturally, the physical insight we gain will be relevant for only that type of system (i.e. other different systems may \emph{happen} to use the same equations), but what we'll see is that the assumptions we make are minimal and can apply to a variety of systems, especially the ones that are considered fundamental. It is indeed surprising how so much can be derived by imposing so little.

TODO: high level view of the derivation


%This work aims to re-organize the known elements and equations in a more consistent and comprehensive way, leading to better insight on why the fundamental concepts and laws are what they are. We will start with physical assumptions, that clarify the models we impose on the physical world, and give arguments on when those assumptions can be considered valid. We will justify our mathematical definitions, based on the formal properties of the objects of our discussion. And will, as a consequence, re-derive known results and theories. We will strive to do this in a way that is mathematically meaningful, philosophically consistent and mathematically precise.


%We'll use concepts from different disciplines, such as set theory, differential geometry, relativity, Hamiltonian and Lagrangian mechanics, and we'll find interesting connections among them. We'll keep names and notation as consistent as possible to current use across the different disciplines. This may sometimes lead to some non sequitur as it will not be immediately clear why the new definitions are equivalent to the standard ones. These are typically resolved by subsequent derivation of the expected properties.

%No mathematical breakthrough should be expected: the goal, after all, is to derive the \emph{known} framework from a set of \emph{simple} definitions in the most \emph{obvious} way possible. No proof is longer than a couple of paragraphs, so the word \emph{theorem} is avoided in favor of \emph{proposition} and \emph{corollary}. The novel, and surprising, result is how so much can be derived from so little.

\section{About}

The work is organized into:
\begin{description}
  \item[Assumptions] these characterize the physical system we are studying and constitute the premise of our discussion. A \textbf{rationale} follows each assumption, which uses physical and sometimes philosophical arguments to motivate why (or why not) such an assumption makes sense (in a particular case).
  \item[Definitions] these encode into mathematical language the physical concepts we are studying. A \textbf{justification} follows each definition, which uses physical and mathematical arguments to explain why such a definition and its properties are necessary to describe a particular physical concept.
  \item[Propositions] these are statements that mathematically follow from definitions or other propositions. A mathematical \textbf{proof} follows each proposition. No mathematical breakthrough should be expected as we mostly use well known results from different fields. The goal is to show how those results make sense in light of the physical assumptions we start with.
\end{description}
This should allow you, the reader, to focus on the parts you are most interested in and skim the rest. The philosophically inclined may focus on the assumptions, their rationale and the definitions. The mathematically inclined may focus on the definitions, the propositions and their proofs. The scientist on the assumptions, the definitions and their justifications, and the propositions.

\section{Style objective}

The writing style should have the following goals:
\begin{description}
  \item[Skip math] one should be able to skip the mathematical definitions/propositions and still get a coherent narrative.
  \item[Well-defined physical objects] each mathematical objects need to be physically well-defined. It must shown to exist and be unique.
\end{description}
Non-goals:
\begin{description}
  \item[Can't skip physics in justifications] justifications of mathematical definitions will inherently be rooted in the preceding physics discussion.
\end{description}
Notation:
\begin{description}
  \item[state variable and space] state space (cont/discr), state variable (cont/discr), possibility (cont/discr), range of possibility (cont/discr), set of possibilities (cont/discr)
\end{description}

\section{On studying a physical system}

% Status: Conceptual work done. Text ready. Incorporated Dave, Isaac and Christine feedback

Our first task is to develop a conceptual model that applies to all realms of physics we'll be considering: classical, statistical and, later, quantum mechanics. We will take the standard picture of system plus environment and extend it to differentiate between the state of the system (i.e. the aspects under study) from the unstated part of the system (i.e. the aspects missing from our description).

We will assume that the state evolves according to a deterministic and reversible law (i.e. for each present state there is one and only one future state). While the unstated part does not influence the state evolution, we'll see how it constrains what states are available and what type of description can or cannot be given to the system.

\subsection{States and their evolution}

We start by fixing a \emph{physical system}, meaning something we can interact with and perform measurements on (e.g. a planet, a fluid, ...). We call \emph{environment} everything else. We set what particular aspect we want to study (e.g. the motion around a star, the flow in a pipe, ...). We call \emph{state} a physically distinguishable configuration of the aspect under study at a particular time (e.g. position/momentum of center of mass, velocity field, ...). Since the state does not, in general, exhaust the description of the system, a part remains \emph{unstated}, and as such we'll call it, for lack of a better word (e.g. the chemical composition, the motion of each of its molecules, ...). Note that the environment plays an essential role here as it's what allows us to define two states as physically distinguishable: we can find an external process (i.e. part of the environment) whose outcome changes depending on the different state. As such processes are what we can use to perform measurements, we consider the experimental apparatus (and us performing measurements) independent of the system, part of the environment.\footnote{This is true even in the case of general relativity, where we can imagine multiple researchers on small spaceships collecting data without greatly influencing the motion of stars and planets. The case where the physical system is the whole universe and there is no environment presents practical problems  and conceptual challenges that the current physical theories do not seem to be equipped to address, and therefore will be absent from our discussion. For example, what physical device can we use to store and process the state of the whole universe to make predictions and compare? How do we define physically distinguishable? Do the physical laws determine which measurements we are going to make and does that limit what is actually distinguishable?}
 
In this context, we call the evolution of the system \emph{deterministic} if the state at a given time uniquely identifies states at future times, and \emph{reversible} if it uniquely identifies states at past times. While this is a common enough definition, we need to be clear how this applies to the unstated part. That is, the evolution of the unstated part under the same process is \emph{always} non-deterministic (and non-reversible) in the sense that the state of the system does not determine its evolution. For example, suppose we define the state as position and momentum of the center of mass of a cannonball. Suppose that the evolution is deterministic on that state. What does it tell us about the cannonball temperature, or about the motion of each of its atoms? Nothing. In this sense, the unstated part is non-deterministic and non-reversible. Could we extend the state and the laws of evolution to account for temperature? Yes, but that would be a different process defined on a different state. Does it mean the unstated part is always evolving chaotically? Not at all. The temperature may remain constant throughout the motion of the cannonball. Yet we wouldn't know, since we are not studying it: the motion is deterministic and/or reversible only as far as the state is concerned. With this in mind, we introduce the following:

\begin{assump}[Determinism and reversibility]\label{detrevass}
The state of the physical system under study undergoes deterministic and reversible evolution.
\end{assump}

\begin{rationale}
As it is an assumption, we first need to discuss when it is valid. More specifically, we need to understand that the non-deterministic/non-reversible evolution of the unstated part plays as much of a fundamental role as the deterministic/reversible evolution of the state. In fact, the non-deterministic part contributes in determining what states are available to the system.

Suppose we study the motion of a cannonball; its state under gravitational and inertial forces will be properly described by the position and momentum of the center of mass. While light and air molecules may scatter off its surface unpredictably, its trajectory is not greatly affected as it is a massive rigid body. Suppose we study the motion of a small particle, small enough that the random scattering does influence the trajectory and it undergoes Brownian motion: its state will be a probability distribution for position and momentum of the center of mass. Gravitational and inertial forces have not changed, yet the set of states has. In other words, the set of states must also be closed under non-deterministic evolution. If the Brownian motion is not negligible, we do not end in a well defined position/momentum pair, even if we start from one.

A similar more drastic effect: consider a book and its motion under gravitation and inertial forces, its state being the position and momentum of the center of mass. As we increase the temperature of the air around the book, its motion remains unaffected until, at some point, the book burns. Clearly, the non-deterministic evolution has pushed one of the states outside the set, to the point that the system is no longer recognizable.

As we have hinted, sometimes the state is identified by a distribution (either statistical or actual). Even in this case, the state can be deterministic and reversible. That is, given the distribution at one time we can determine the distribution at future times. The shape and the parameters of the distribution can be deterministic, even if the trajectories of the parts are not as they fall within the unstated part. Note that we \emph{cannot} assume trajectories and states are always defined for the unstated part, as this includes also the unknown unknowns. We will return to this aspect when discussing quantum systems.

It should also be clear that what constitutes state and unstated part does not depend only on the system under study, but also on the processes we are considering. In some circumstances, the chemical composition of a fluid may be relevant, in others it may not. The choices of environment, state and unstated part are not independent from each other. By choosing a particular set of states, we are not only saying that the state evolution is well approximated by a deterministic/reversible map from initial to final state, but we are also saying that the non-deterministic/non-reversible evolution of the unstated part does not change the nature of the system, and processes that do not satisfy these conditions are not under consideration.

As with all assumptions, we should also ask whether it is necessary. That is, could we define a set of physically distinguishable states and yet have no deterministic and reversible processes defined on them? The claim is that this assumption is indeed needed, as without it we cannot properly define states or write useful physics laws. We can provide different arguments that point in the same direction.

First, to be able to identify the system, we must be able to tell it apart from anything else. Intuitively, we can distinguish between two chairs because we can move the first to another room and sit on it without having touched the second. We can manipulate the state of the first system without affecting the second, and vice-versa. So, to identify a system it has to be sufficiently isolated from everything else. This means that the system future and past states are with good approximation determined only by its own state: the state undergoes deterministic and reversible evolution.

Second, the aim of physics is to write laws that can be used to make predictions that can be validated experimentally. If I drop an anvil from a tower, it will accelerate at $9.81$~m/s$^2$; if I want the anvil to reach the ground at $x$~m/s I have to drop it from $y$~m. To the extent that we want to make predictions in time, we need to have a one-to-one correspondence between initial and final states.

Third, operationally we must reliably prepare and measure states. That is, we need a process for which the input settings of our preparing device determine the outgoing state of the system; and a process for which the incoming state of the system can be reconstructed by the output of the measuring device. That is, our system must participate in a deterministic and reversible process with the preparing and measuring device. Without it we wouldn't be able to calibrate our experimental apparatus.

Fourth, to be able to ascribe a property to a system we need to claim that, at least for a finite interval of time, the system either held or did not hold such property. That is, there is a deterministic and reversible process for that finite period of time for such property.

This link between state definition and deterministic processes should not be too surprising as the state, in the context of thermodynamics and systems theory, is often defined as \emph{the set of variables needed to determine the future evolution of the system}. As we saw before, this applies also to statistical processes: the distribution (the ensemble) as a whole can indeed be calculated, measured and prepared. We can also describe the evolution of each element of the distribution provided that: we have a way to isolate it and study it under deterministic and reversible motion (so that we can define microstates); the non-deterministic motion does not alter the system (the set of microstates is preserved by the evolution).

As with many assumptions, we should stress that it's an idealization: it can never be completely achieved in practice. A system can be prepared or measured up to a certain level of precision. Perfect determinism and isolation of a system is impossible both practically (e.g. black-body radiation, gravity, ...) and conceptually (e.g. if the system is perfectly isolated, we cannot interact with it: how can it be physically distinguished?). It's a simplifying assumption that can only be taken if the environment and the internal dynamics of the system interact in such a way that they little affect and are little affected by the aspect we are studying. As we saw before, for example, assuming that the state consists of the position and momentum of the center of mass requires assuming that the Brownian motion of the body is negligible.

Yet, this is a fundamental assumption in the sense that it is needed. If a particular set of states does not satisfy deterministic and reversible evolution under certain conditions, what we do is to keep at it until we find a set that does. That is, we work to restore the assumption. Finding new sets of states with new laws of evolution is, in fact, what leads to new physics. We will therefore call \emph{fundamental model of physics} the triad of state, unstated part and environment, with the assumption that the state may undergo deterministic and reversible evolution, and the unstated part undergoes non-deterministic non-reversible evolution that does not alter the set of states.
\end{rationale}

\section{States and state space}

% Status: First subsection. Conceptual work done. Text ready. Incorporated Christine feedback
% Second subsection. Conceptual work almost done. Need to finish the text

%Open questions: smoothness (obviously required to define densities later, is it required in general? What does it means that physical processes/measurements don't give a smooth topology?)

We now proceed to characterize states and physically distinguishability in more precise terms so that we can capture their description mathematically. What we'll see is that the outcomes of all physical processes that can be used to gain information about the system, that can be used to perform a measurement, induce a topology on the set of states that is Hausdorff and second countable. That is, physical distinguishability is mathematically captured by topological distinguishability. Deterministic and reversible evolution will then preserve the topology, and they will be mathematically captured by self-homeomorphisms.

We'll focus on state spaces that can be described by a set of independent state variables, either discrete or continuous, and see what can be said in general on the evolution of state variables.

\subsection{States and topology}

As the term "measurement" has become particularly loaded, let's first characterize what we mean by physical distinguishability in our context.

Consider the motion of a cannonball under inertial and gravitational forces. Light will scatter off of it; as it lands the ground will be deformed and the impact will make the temperature slightly rise. Those external physical processes, which happen no matter what we do, can be used to distinguish the motion of the cannonball as their outcomes are correlated. Therefore we can learn the position by looking at the reflected light, and learn the final kinetic energy by looking at the deformation of the ground. In principle, any external process that has a correlation with the states under study can be used to perform a measurement, and any measurement is based on such a process. That is, for us a measurement is simply a physical process that we can use to distinguish states. Setting up an experiment means choosing a particular process with desired outcomes and forcing the system under study to interact with it one or more times. After that, there is no special role played by the "observer" in making outcomes come about.

This external process may be quite complicated: when a particle enters a calorimeter, a shower of particles is produced, photons are captured and are directed to photomultipliers, a current is read out, the current is then digitized, and so on. Some processes interfere with the system, they affect its dynamics, and others are destructive, the system no longer can be described by the original set of states. A tracking chamber is an example of the first (the magnetic field curves the motion of a charged particle); burning a substance to determine its caloric content is an example of the latter. Therefore intimate knowledge of the process is always needed to ensure that one makes the proper link between outcomes and the \emph{original} states, and properly accounts for systematic uncertainties that would skew that link.

Repeatability is also fundamental. First, to make sure the process is indeed correlated to the states. Second, because "a single take does not a measurement make". One has to gather enough statistics. Note that the number of takes influences the outcomes: with greater statistics the precision and number of distinguishable cases increases. Therefore the processes, as we defined them, may require repeated interactions with similarly prepared states. They may even be a combination of different kinds of interactions that, taken all together, create a set of distinguishable outcomes. But in the end, however complicated it is, the conceptual model remains the same: each process has a set of possible outcomes, and each possible outcome will be associated with a set of states consistent with that outcome. For example, if the cannonball deformed the ground by this amount then its kinetic energy at impact was within this range; if the electron follows a certain path, then its state is among the ones that have spin $+1/2$.

However precise our measurements are, we can only gather a finite amount of statistics and each outcome is expressed by a finite set of digits; therefore, the set of outcomes is countable.\footnote{The information provided by the process as measured by Shannon's entropy is finite.} Note that different outcomes for the same process can overlap. For example, $4.12 \pm 0.05$ cm and $4.13 \pm 0.05$ cm are both legitimate possible outputs of the same measurement device. But since all states must be distinguishable, given two arbitrary states there must be a process precise enough to tell them apart. That is, the potential outcomes associated with the two states do not overlap. For example, $4.12 \pm 0.0005$ cm and $4.13 \pm 0.0005$ cm do not overlap anymore.

We can also conceptually combine two different processes into a single one. That is, having a way to measure quantity $x$ and a way to measure quantity $y$ gives us a way to measure the combination $(x,y)$. For ensembles, if we can measure the marginal distribution $\rho_x(x)$ and the marginal distribution $\rho_y(x)$, we know the joint distribution $\rho(x,y)$ has to be compatible with both. Note, though, that this does not provide a way to fully measure $\rho(x,y)$ as we know nothing about the correlation between the two variables.\footnote{The quantum case is similar: we measure marginal distributions and rule out states that are incompatible with them.} Formally, the states compatible with the outcomes of the combined process will be the intersections of the states compatible with each pair of outcomes of the original processes.

We can also coarsen a single process. That is, having a way to measure $(x,y)$ gives us a way to measure $x$ alone (or any $f(x,y)$). Formally, the outcomes of the coarsened process are given by performing the union of some outcomes of the original process.

This model maps very naturally to a topological space. The states are elements of a set and the physical outcomes provide a topology on that set. The fact that two elements of the set can be distinguished requires the space to be Hausdorff. The fact that the potential outcomes of a process are countable requires the space to be second countable.

\begin{defn}\label{statedef}
The state space $\mathcal{S}$ of a physical system is a Hausdorff and second countable topological space.
\end{defn}

\begin{justification}
We claim $\mathcal{S}$ is a set. Each state is well defined as it is physically distinguishable. The collection of all possible states form a set.

We claim $\mathcal{S}$ has a topology $\mathsf{T}$. Consider the set of all possible physical outcomes associated with all physical processes. Each possible outcome is associated with a set of states that are compatible with that outcome. Let $\mathsf{T}$ be the set of all sets associated with all physical outcomes. $\varnothing \in \mathsf{T}$ and is associated with impossible outcomes. $\mathcal{S} \in \mathsf{T}$ and is associated with unavoidable outcomes. Let $V_1, V_2 \in \mathsf{T}$. Then, by definition, there exist a process $P_1$ that admits $V_1$ as an outcome and a process $P_2$ that admits $V_2$ as an outcome. Consider the process $P$ that combines the outcomes of $P_1$ and $P_2$. This always exist physically as we can prepare the same state multiple times and let it interact with each process separately. $P$ will have a possible outcome $V$ corresponding to the case where $P_1$ gave outcome $V_1$ and $P_2$ gave outcome $V_2$. The states compatible with $V$ must be in both $V_1$ and $V_2$, that is $V = V_1 \cap V_2$. Therefore $\mathsf{T}$ is closed under intersection. Let $V_1, V_2 \in \mathsf{T}$. If they are physically distinguishable, then there exists a physical process $P$ that admits both as outcomes. Given $P$, we can always construct the process $P_0$ that combines $V_1, V_2$ into a single outcome $V$ by "forgetting" which of the two was given. The states compatible with $V$ must be in either $V_1$ or $V_2$, that is $V = V_1 \cup V_2$. Therefore $\mathsf{T}$ is closed under union. $\mathsf{T}$ is a topology by definition.

We claim that $\mathcal{S}$ is Hausdorff. Let $\mathcal{s_1}, \mathcal{s_2} \in \mathcal{S}$. As states are physically distinguishable, there must exist a physical process with two outcomes $V_1, V_2 \in \mathsf{T}$ for which $s_1 \in V_1, s_2 \in V_2, V_1 \cap V_2 = \varnothing$. $\mathcal{S}$ is Hausdorff by definition.

We claim that $\mathcal{S}$ is second countable. Consider the set of outcomes of a process. As each is identified by a finite number of digits, the set of outcomes is countable. This remains true when combining processes: the number of processes we can combine is finite as is the number of times we can prepare the same state and the intersections of countably many sets is still countable. Consider a basis for the topology. Suppose it is of cardinality greater than countable. Then no physical process could ever distinguish between the elements of the basis. The basis, and therefore the outcomes, would not be physically distinguishable. The basis for the topology cannot be of cardinality greater than countable. By contradiction, $\mathcal{S}$ must be second countable.

\end{justification}

Note that the arguments that lead to the topological space had nothing to do with states per se, just that they are physically distinguishable. States, though, are not the only objects with that property. In fact: any element of a set of physical objects (e.g. forces, physical properties such as mass or charge, time) needs to be physically distinguishable to be well defined. We can generalize the above justification: any set of physically distinguishable elements is a topological space.

\begin{defn}\label{topologically_distinguishable}
	Any set $\mathcal{S}$ of physically distinguishable elements is a Hausdorff and second countable topological space.
\end{defn}

\begin{justification}
	Same justification as in \ref{statedef} with "state" replaced by "element" of the set $\mathcal{S}$.
\end{justification}

With our state space defined, deterministic and reversible evolution corresponds to a bijective map between initial and final states. Moreover, a physical process that can distinguish final states is also a physical process that can distinguish initial states. That is: the topology is mapped and preserved by the deterministic and reversible evolution (i.e. initial and final states are equally distinguishable). The evolution is then a self-homeomorphism on the state space.

\begin{defn}\label{detrevmap}
A deterministic and reversible evolution map is a self-homeomorphism on the state space $\mathcal{T}_{\Delta t}:\mathcal{S} \rightarrow \mathcal{S}$.
\end{defn}

\begin{justification}
We claim that $\mathcal{T}_{\Delta t}$ exists. The system is deterministic: given an initial state $\mathcal{s} \in \mathcal{S}$ there exists a well defined final state $\mathcal{T}_{\Delta t}(\mathcal{s})=\hat{\mathcal{s}} \in \mathcal{S}$.

We claim $\mathcal{T}_{\Delta t}$ is continuous. Let $U \subseteq \mathcal{S}$ represent an outcome of a process $P$ on the final states. $U$ is an open set in the (final) state space topology by definition. Consider the process $P_0$ that evolves the initial states and then distinguishes the final state with $P$. $P_0$ is a process that distinguishes initial states. The set of initial states compatible with $U$ are $\mathcal{T}_{\Delta t}^{-1}(U)$. $\mathcal{T}_{\Delta t}^{-1}(U)$ is an open set in the (initial) state space topology by definition. $\mathcal{T}_{\Delta t}$ is a continuous map.

We claim that $\mathcal{T}_{\Delta t}$ is a bijection. The system is reversible: there exists a map $\mathcal{T}_{-\Delta t}:\mathcal{S} \rightarrow \mathcal{S}$ that returns the initial state given the final state. $\mathcal{T}_{-\Delta t} \circ \mathcal{T}_{\Delta t} = \mathcal{T}_{\Delta t} \circ \mathcal{T}_{-\Delta t} = id$ as mapping forward and then backward or backwards and then forward must return the original element. $\mathcal{T}_{\Delta t}$ admits $\mathcal{T}_{-\Delta t}$ as an inverse. $\mathcal{T}_{\Delta t}$ is a bijection.

We claim that $\mathcal{T}_{\Delta t}$ is a self-homeomorphism as it is a continuous bijection.
\end{justification}

Again, we note that the arguments that lead to continuity had nothing to do with states per se, just that the relationship is between physically distinguishable objects. We can then generalize the above justification.

\begin{defn}\label{continuous_map}
	A map $f:\mathcal{S_1} \rightarrow \mathcal{S_2}$ between two sets of physically distinguishable elements $\mathcal{S_1}$ and $\mathcal{S_2}$ is a continuous map.
\end{defn}

\begin{justification}
	Same justification for continuity as in \ref{detrevmap} with "initial states" and "final states" replaced by "elements" of $\mathcal{S_1}$ and $\mathcal{S_2}$ respectively.
\end{justification}

The generality of this result explains why in physics one always assumes functions to be well behaved. As the result was derived from our notion of physical distinguishability, this is not a matter of practical convenience. Suppose we were able to prepare a force field that was zero everywhere in space except at a single point. This gives us a way to tag a specific point. But it also allows us to create an outcome compatible with only a single point. A finite precision measurement of the force provides us infinite precision of space, which we ruled out.\footnote{This assumes that we are able to position the probe perfectly, which we could do if we were able to manipulate forces at that precision.} This is what the math is telling us, that if we said before that outcomes can't distinguish isolated points (i.e. the topology is second countable) then neither can maps. It is physical consistency that limits us to continuous functions.

While functions with few discontinuities are useful and used in physics and engineering, they are employed for idealized cases  (e.g.~a signal change is fast enough, a charge distribution is small enough) that are often treated as a special case (e.g.~propagation in material discontinuities). While this may be obvious and intuitive to the physicist, it may be troubling to the mathematician as the proper use of many mathematical techniques requires the inclusion of discontinuous functions. This is less of a problem than it would seem at first. Once we made sure that the objects and their relationships are physically meaningful, we can extend our mathematical spaces for the purpose of math computations. Our physically meaningful continuous function can be expanded into a sum of discontinuous functions. One just has to be mindful of the extension and be wary that mathematical results that depend on such extension may or may not be physically meaningful.

\subsection{Manifolds and labeling states}

To identify and name states one uses a set of quantities, typically numbers. For example, the orbital of an electron in a hydrogen atom is identified by the quantum numbers $n$, $l$, $m$ and $s$. We call each of these quantities \emph{state variables}. We call a \emph{possibility} a possible value that can be taken by a state variable. We call a state variable \emph{discrete} or \emph{continuous} if the possibilities are integer or real numbers respectively. From now on, we are going to consider state spaces whose states can be identified, at least within a region, by a finite set of discrete and continuous state variables (i.e. the state space is locally isomorphic to $\bigcup\limits_{1 \leq i \leq n} \mathbbm{R}^{m_i}$).\footnote{TODO: Is there a more physically meaningful requirement? Could we have a set of states that are not locally described by a set of quantities?}

We purposely use the term state variables instead of coordinates (even though that's what they are mathematically) as it would create confusion with space-time coordinates. We also avoid the term observable or measurable, as not all state variables may be directly physically tangible (e.g. conjugate momentum in a gauge theory). The only requirements for state variables is that they identify states. This means the possible values for state variables are physically distinguishable and are defined at equal time (since states are defined and mapped at a particular time). \footnote{This is the main reason that a quantity like velocity is not a suitable state variable, as it is  defined over an interval of time. Therefore velocity is always physically well defined but is not a state variable in general, while conjugate momentum is always a state variable but is not physically well defined by itself (it requires the vector potential to be specified as well). As we'll see later, if there exists a one to one map between velocity and conjugate momentum they can both be physically well defined state variables.}

\begin{defn}\label{state_variable}
	A \emph{state variable} is a continuous map $q : U \rightarrow \mathbbm{L}$ where $U \subseteq \mathcal{S}$ and $\mathbbm{L}$ is the space for the possible values. If $\mathbbm{L}\cong \mathbbm{Z}$ the variable is said \emph{discrete}. If $\mathbbm{L}\cong \mathbbm{R}$ the variable is said \emph{continuous}.
\end{defn}

\begin{justification}
	We claim $\mathbbm{L}$ is a topological space. $\mathbbm{L}$ is a set of physically distinguishable possibilities. $\mathbbm{L}$ is a topological space because of \ref{topologically_distinguishable}.
	
	We claim $q$ is a continuous map. $q$ is a map between two sets of physically distinguishable elements. $q$ is continuous because of \ref{continuous_map}.
\end{justification}

When combining multiple state variables, it is important to understand how they relate to each other. Consider the orbital of an electron in a hydrogen atom, which is identified by the quantum numbers $n$, $l$, $m$ and $s$. For each combination of $n$, $l$ and $m$, the spin $s$ can have two values. The choice of spin is independent from the rest. The choice of $l$, though, depends on the choice of $n$: for $n=1$ only $l=s$ is available; for $n=2$ we can choose $l=s$ or $l=p$. The choices are not independent. That is: two or more variables are independent if there always exists a state for any possible combination, if the total number of states is the cartesian product of the possibilities of each variable. We call \emph{state vector} a collection of independent state variables that fully identify a state.

\begin{defn}\label{independent_state_variables}
	Two state variables $q_1 : U \rightarrow \mathbbm{L}_1$ and $q_2 : U \rightarrow \mathbbm{L}_2$ are said \emph{independent} if $\exists \mathcal{s} | q_1(\mathcal{s})=l_1, q_2(\mathcal{s})=l_2 \forall l_1 \in q_1(U), l_2 \in q_2(U)$.
\end{defn}

In general, the entire state space may not be identified by a predetermined set of independent state variables. Consider the state of a pool table, determined by the number of balls together with position and momentum of the center of mass: the number of state variables is not fixed as it depends on the number of balls. But deterministic and reversible evolution cannot take us from a different number of continuous independent state variables (i.e. there is no homeomorphism between $\mathbbm{R}^n$ and $\mathbbm{R}^m$). That is the number of balls cannot change under deterministic and reversible evolution. Therefore we can restrict ourselves to the case where the number of continuous independent state variables is constant without loss of generality. This means that, at least locally, the state space is always homeomorphic to $\mathbbm{R}^n$, and is therefore a manifold.

Discrete variables do not present such problems, as any number of them can be flattened out in a single one (i.e. $\mathbbm{Z}^n$ is homeomorphic to $\mathbbm{Z}$). They will determine the number of connected components of the state space. Once we introduce a continuous parameter for time evolution, though, these become irrelevant as continuous time evolution requires continuous trajectories that cannot move states across disconnected components. This justifies the special interest in path connected manifolds, as this is where trajectories for deterministic and reversible continuous evolution live.

\begin{prop}\label{continuous_state_space}
	Let $\mathcal{s} \in \mathcal{S}$ a state within a state space. The set of states $\mathcal{S}'\subseteq\mathcal{S}$ potentially reachable from $\mathcal{s}$ by deterministic and reversible continuous evolution is a path connected manifold of dimension equal to the number of independent continuous state variables necessary to identify it.
\end{prop}

\begin{proof}
	We claim $\mathcal{S}'$ is a manifold. Let $\mathcal{s} \in \mathcal{S}$. Let $n$ be the number of independent continuous variables needed to identify $\mathcal{s}$. There exists a neighborhood $U$ around $\mathcal{s}$ where we have $(q_1,...,q_n):U\rightarrow \mathbbm{R}^n$. This map is a bijection as $\mathcal{s}$ is identified by those variables. $\mathcal{S}$ is homeomorphic to $\mathbbm{R}^n$ around  $\mathcal{s}$. Let $\mathcal{S}'$ be the set of all states potentially reachable by deterministic and reversible evolution from $\mathcal{s}$. Deterministic and reversible evolution is a homeomorphism between initial and final states. $\forall \hat{\mathcal{s}} \in \mathcal{S}'$ there exist a map $\mathcal{T}_{\Delta t}:\mathcal{S} \rightarrow \mathcal{S}$ such that $\hat{\mathcal{s}} =\mathcal{T}_{\Delta t}(\mathcal{s})$. $\hat{\mathcal{s}} \in \mathcal{T}_{\Delta t}(U)$ and $\mathcal{T}_{\Delta t}(U)$ is isomorphic to $\mathbbm{R}^n$. $\mathcal{S}'$ equipped with the subspace topology is a Hausdorff, second countable topological space everywhere homeomorphic to $\mathbbm{R}^n$. $\mathcal{S}'$ is a manifold of dimension $n$.
	
	We claim $\mathcal{S}'$ is path connected. Let $\hat{\mathcal{s}} \in \mathcal{S}'$. There exist map $\lambda : [t_0,t_1] \rightarrow \mathcal{S}'$ where $\lambda(t_0)=\mathcal{s}$, $\lambda(t_1)=\hat{\mathcal{s}}$ and $t_0$ and $t_1$ are the initial and final time respectively. $\lambda$ is a map between two physically distinguishable quantities, time and states. $\lambda$ is continuous by \ref{continuous_map}. All $\hat{\mathcal{s}} \in \mathcal{S}'$ are path connected to $\mathcal{s}$. $\mathcal{S}'$ is path connected.
\end{proof}

\subsection{Evolution of state variables}

To study time evolution, we need to describe how state variables change under deterministic and reversible evolution. There are two ways to do it, and we'll need both. The first approach is to \emph{evolve} the state variables from the initial value $q^i(\mathcal{s})$ to the final value $q^i(\hat{\mathcal{s}})$. The result is a trajectory $q^i(t)=q^i(\lambda(t))$ which is especially useful when state variables correspond to physically meaningful quantities. For example, we track how the temperature or the pressure of an ideal gas changes. Evolved state variables, however, make it hard to find relationships that are invariant and common to all deterministic and reversible processes.

The second approach is to transport the state variables. The idea is to keep the connection to the initial state by labeling the future state, instead by the future values of $q^i$, by the original one. For example, the evolved state variable will not tell us the current value of pressure, but the one for the initial conditions. So we introduce a new set of variables for which $\hat{q}^i(\hat{\mathcal{s}})=q^i(\mathcal{s})$, which we can always do as the evolution is deterministic and reversible. The level sets of $\hat{q}^i$ are the evolved level sets of $q^i$ (e.g. all the states that started with a particular value for pressure) which allows us to study how groups of states evolve in time. Given that the value of the transported state variables does not change during evolution, and that it is unique for each initial state, transported state variables also provide a way to label the trajectories themselves.

Evolved state variables are useful to write equations of motions, study how physical quantities change, form a physical picture of what happens. Transported state variables are useful to write invariants, study state space trajectories, form a geometric picture for the state space.\footnote{One should not confuse evolved/transported state variable with active/passive transformations or with Schroedinger/Heisenberg pictures. In those cases, the choice is between changing the state or the coordinates/observables. In our case, the state is always changing. The choice is between tracking the change using different state variables or different values.}

\begin{defn}\label{evolved_transported_variable}
Let $q^i$ be a set of state variables and $\mathcal{T}_{\Delta t}$ a deterministic and reversible evolution map. The evolved state variables are given by $q^i \circ \mathcal{T}_{\Delta t}$. The transported state variables are given by $q^i \circ \mathcal{T}_{-\Delta t}$.
\end{defn}

To show the usefulness of transported state variables, we can prove the following:

\begin{prop}\label{discrete_state_metric}
	Let $U \subseteq \mathcal{S}$ a set of states fully identified by a set of $n$ independent discrete state variables $q^i$. Let $\Delta q^i = (q^i)^{-1}(U)$ the range of possibilities of each variables. Then $\#(\Delta q^i)=\#(\Delta \hat{q}^i) \; \forall i$ and $\#(U)=\prod\limits_{i=1}^{n}\#(\Delta q^i)=\prod\limits_{i=1}^{n}\#(\Delta \hat{q}^i)=\#(\hat{U})$ where $\hat{U}=\mathcal{T}_{\Delta t}(U)$.
\end{prop}

\begin{proof}
	We claim $\#(\Delta q^i)=\#(\Delta \hat{q}^i)$. For each $\mathcal{s} \in U$, let $\hat{\mathcal{s}}=\mathcal{T}_{\Delta t}(\mathcal{s})$. We have $\hat{q}^i(\hat{\mathcal{s}}) = q^i(\mathcal{s})$. Therefore $\Delta q^i = (q^i)^{-1}(U) = (\hat{q}^i)^{-1}(\hat{U})=\Delta \hat{q}^i$. $\#(\Delta q^i)=\#(\Delta \hat{q}^i)$.
	
	We claim $\#(U)=\prod\limits_{i=1}^{n}\#(\Delta q^i)$. As $q^i$ are independent variables, the states are the Cartesian product of the possibilities of each variable.
\end{proof}

What happens is that, because the variables are independent, the total number of states is the product of the number of possibilities for each variable. If $\Delta q^1$ and $\Delta q^2$ are ranges of possibilities for two independent variables, the total number of possibilities is $\#(\Delta q^1) \#(\Delta q^2)$. By construction, the set of possibilities for each independent variables $\Delta q^i$ is the same as the set of possibilities for the transported variable $\Delta \hat{q}^i$. Also, the transported variables remain independent. Therefore the relationship $\#(\Delta \hat{q}^1) \#(\Delta \hat{q}^2) = \#(\Delta q^1) \#(\Delta q^2)$ is valid throughout the evolution.

We'll see that very similar relationships are what define Hamiltonian and Lagrangian mechanics. But recovering them in the continuous case is trickier, and in fact will be part of the challenge in the following sections. Consider the map $q'=aq$ with $0<a<1$. At first glance, it's a bijective continuous map so we may think it can represent a deterministic and reversible evolution. Yet, $\Delta q = [-b, b] \supset \Delta q'$: a set of states is mapped to a proper subset (i.e. to fewer states) which does not make sense for a reversible process. In the limit where we apply the map an infinite amount of times, any value of $q$ will be brought infinitely close to $0$, which also does not sound like a reversible process. The issue is that while deterministic evolution will map points to points, not all point to point maps can be considered deterministic and reversible. Discrete sets already come with a way to measure and compare the number of elements which is preserved by any bijective map. With continuous sets, we need to do extra work to properly define a measure (or metric) that allows to count states and possibilities.

\section{Composite systems and reducibility}

%Status: conceptual work is done, need to write down

Given that scientific reductionism (i.e. the idea of reducing physical systems and interactions to the sum of their constituent parts in order to make them easier to study) is at the heart of fundamental physics, we now explore how to characterize a system in terms of its components. That is, we want to study the relationship between the state space $\mathcal{C}$ of the composite system and the state space $\mathcal{S}$ of its parts. In general, this is quite a complicated thing to do, that requires intimate knowledge of the system at hand. So we simplify our problem and study an idealized case: one where the components are homogeneous and infinitesimal. What we'll find is that under the additional assumption that the system is infinitesimally reducible (i.e. its state is equivalent to the state of its parts) and that each part undergoes deterministic and reversible evolution, the motion is suitably described by the standard framework of classical Hamiltonian particle mechanics.

\subsection{Homogeneous decomposable systems}
The notion that a system is decomposable means the states is equipped with a rule of composition that allows to write $\mathcal{c}=\mathcal{c}_1+\mathcal{c}_2$: the composite system is the sum of its part. For example, the state of a ball is equal to the state of its left and right parts.

The notion that the system is homogeneous means that the states of the composite and of the parts are not unrelated: they are made of the same material.\footnote{Whether a system is homogeneous depends on context (e.g. air can be thought as homogeneous if the mixture of gases does not change in space or in time due to phase transitions or chemical processes) and one must check that such property is maintained by by time evolution.} In fact, the state space $\mathcal{C}$ of all systems composed of such material will the state of the system as well as the states of all its part (i.e $\mathcal{c}, \mathcal{c}_1, \mathcal{c}_2 \in \mathcal{C}$). Since combining any two systems made of a homogeneous material will always give us a system made of the same homogeneous material (e.g. combining elements made of water gives us another element made of water), the state space $\mathcal{C}$ is closed under composition.

As we study the system under evolution, we'll also want to study state changes. For example, if a stable mixture of gas expands, we'll want to know the difference between the initial and final distributions. That is: $\Delta\mathcal{c}=\hat{\mathcal{c}}-\mathcal{c}$. Note that such state difference may not describe a physical state: it may remove some material from one location to add it somewhere else. Yet, these state changes are still physically distinguishable objects that provide a configuration for the same homogeneous material, therefore we extend the state space $\mathcal{C}$ to include state differences. All combined, this gives us the structure of an abelian group.

\begin{defn}\label{reducible_state_space}
The state space $\mathcal{C}$ for decomposable systems made of a homogeneous material is an additive abelian (i.e. commutative) group.
\end{defn}

\begin{justification}
We claim $\mathcal{C}$ is an additive monoid. There exist a law of composition $+ : \mathcal{C} \times \mathcal{C} \rightarrow \mathcal{C}$ that takes two states and returns one that is the physical composition of the two. The domain and codomain match because the material is homogeneous. The law is commutative $\mathcal{c}_1 +\mathcal{c}_2 = \mathcal{c}_2+\mathcal{c}_1$ and associative $(\mathcal{c}_1 + \mathcal{c}_2) + \mathcal{c}_3 = \mathcal{c}_1 + (\mathcal{c}_2 + \mathcal{c}_3)$, as it does not matter in what order we physically compose the parts. There exist a unique zero element $\mathcal{c} + 0 = \mathcal{c}$ and it represent the physically empty state (i.e. no amount of material).

We claim $\mathcal{C}$ is an additive group. As we want to describe changes during the evolution (i.e. $\hat{\mathcal{c}} = \mathcal{c} +\Delta \mathcal{c}$) we introduce an inverse $- : \mathcal{C} \rightarrow \mathcal{C}$ such that $\mathcal{c} + ( - \mathcal{c}) = 0 \forall \mathcal{c} \in \mathcal{C}$. Such inverse may introduce objects that do not properly represent a physical state, but a state change. A change of a physically distinguishable object is itself physically distinguishable therefore $\mathcal{C}$ is still a Hausdorff and second countable topological manifold as per \ref{statedef}. We'll still call $\mathcal{C}$ state space committing an abuse of terminology.
\end{justification}

\subsection{Classical homogeneous systems}
We also want to be able to express the state of the composite system in terms of the state of the parts. For example, given the state of a fluid we'll want to know how its parts are distributed. This in general will depend on how much the state of the composite system "knows" about its parts. For example, the position and orientation of an ideal rigid body is enough to define where all its constituents are, while the volume/pressure/temperature of an ideal gas is not enough to determine the position and momentum of all its molecules. Therefore we need to characterize the system further.

We will call a classical system one that is infinitesimally reducible. That is: it is made of arbitrarily small parts, which we call particles, and the state of the whole system is equivalent to the sum of the states of the parts. Given the state space $\mathcal{S}$ for the infinitesimal parts, a composite state $\mathcal{c}$ will tell us the amount of material for each possible particle state. That is, each state is fully identified by a function $a: \mathcal{S} \to \mathbbm{R}$ that returns the amount of material in the composite state $\mathcal{c}$ that is prepared in state $\mathcal{s}$. For example, given a certain configuration of gas, we can tell the amount that is at a specific point in space with a specific value of momentum. The notion that the parts are infinitesimal requires the co-domain of $a$ to be continuous, as opposed to discrete. The state space $\mathcal{S}$, instead, can be finite. For example, for a system composed of different tanks connected by pipes, the state of the composite system could be the overall distribution of water among the tanks. The amount in each tank is continuous (as we assume the water to be infinitesimally divisible) yet there are only a finite number of tanks the water can be placed into.

As we combine the state of finite parts, we sum the distributions over particle states $a(\mathcal{s})=a_1(\mathcal{s})+a_2(\mathcal{s})$: the amount of material in the composite state is the sum of the amount of material of the parts. This gives us the structure of a real vector space.

\begin{defn}\label{classical_vector space}
The state space $\mathcal{C}$ for a homogeneous classical (i.e. infinitesimally reducible) material is a vector space over $\mathbbm{R}$. It admits a basis isomorphic to the state space $\mathcal{S}$ of its particles (i.e. infinitesimal parts).
\end{defn}

\begin{justification}
The state space is an abelian group as the material is homogeneous and decomposable and \ref{reducible_state_space}.

We now consider the set of transformations $T$ that increase or decrease the amount of material in the system by a constant. We claim $T$ is a field\footnote{Here field is intended in the abstract algebraic sense (a nonzero commutative division ring) which has no relationship to the field in the physics sense (a physical quantity with a value for each point in space).} isomorphic to $\mathbbm{R}$. Consider $\tau: \mathbbm{R} \rightarrow T$ the mapping between a number and the transformation that increases or decreases the amount of material by that number. This transformation exists: the system is infinitesimally decomposable and the amount can be changed continuously. Define on $T$ an addition $+: T \times T \rightarrow T$ and a multiplication $*: T \times T \rightarrow T$ such that $\tau(a) + \tau(b) = \tau(a+b)$ and $\tau(a) * \tau(b) = \tau(a*b)$, $a,b \in \mathbbm{R}$, so that the sum and product of the transformation is equal to the sum and product of their respective factors. $\tau$ is an isomorphism between $T$ and $\mathbbm{R}$ as fields.

We now claim that the state space $\mathcal{C}$ is a vector space over $\mathbbm{R}$. The abelian group $\mathcal{C}$ can be extended with the operations defined by $T$, as each element $\tau \in T$ is a map $\tau : \mathcal{C} \rightarrow \mathcal{C}$. The map has the following properties: $(\tau_1 + \tau_2) \mathcal{c} = \tau_1 \mathcal{c} + \tau_2 \mathcal{c}$, increasing the amount of material by the sum of two constant is the same as combining the separate increases, and $\tau (\mathcal{c}_1 + \mathcal{c}_2) = \tau \mathcal{c}_1 + a \mathcal{c}_2$, increasing the amount of the total system is the same as the combination of the increased parts. $\mathcal{C}$ is a module over $T$, which is a field and isomorphic to $\mathbbm{R}$. $\mathcal{C}$ is (isomorphic to) a real vector space.

Consider now the injection map $\mathcal{e}: \mathcal{S} \hookrightarrow \mathcal{C}$ that for each particle state returns a composite state constituted only by a predetermined unit amount of material in such state. The image of such injection is a basis for $\mathcal{C}$. We first claim that all elements are linearly independent. Consider $\sum\limits_{\mathcal{s} \in \mathcal{S}} \tau(a(\mathcal{s})) \mathcal{e}(\mathcal{s})$, $a: \mathcal{S} \to \mathbbm{R}$. This corresponds to a physical state with $a(\mathcal{s})$ material in each state $\mathcal{s}$. This is equal to the empty state if and only if $a(\mathcal{s})=0 \forall \mathcal{s} \in \mathcal{S}$. We now claim they span. Consider $\mathcal{c} + \sum\limits_{\mathcal{s} \in \mathcal{S}} \tau(a(\mathcal{s})) \mathcal{e}(\mathcal{s})$. This can always be made equal to the empty state by setting $a(\mathcal{s})$ to be the opposite of the amount of material of $\mathcal{c}$ found in state $\mathcal{s}$. Both arguments hold even if $\mathcal{S}$ is an infinite or uncountable set, as they hold for each finite subset of $\mathcal{S}$ independently and therefore $\mathcal{C}=\prod\limits_{\mathcal{s} \in \mathcal{S}} span({\mathcal{e}(\mathcal{s})})$.
\end{justification}

Need to define inner product. Introduce $Q:\mathcal{C} \to \mathbbm{R}$ amount of "stuff" of the system. $Q:\mathcal{S}\times\mathcal{C} \to \mathbbm{R}$ amount of stuff of the composite system in that particular state. We can use those those to construct inner product, and to show that states are bounded, but: norm induced by the inner product is not physical.

\begin{defn}\label{classical_inner_product}
	TODO: there exist an inner product.
\end{defn}

\begin{justification}
	TODO
\end{justification}

Show countable basis. $\mathcal{c}= \sum\limits_{\mathcal{s} \in \mathcal{S}} a(\mathcal{s}) \mathcal{e}(\mathcal{s})=\sum\limits_{q \in \mathbbm{Q}} a(q) \mathcal{e}(q)$

% Continuous state variable: Q is measured at constant t

Uncountable basis does not work. $\mathcal{c}= \int\limits_{q \in \mathbbm{Q}} \rho(q) \mathcal{e}(q)dq$ but $\rho(q')=|\partial _{q}q'|\rho(q)\neq\rho(q)$

TODO: need to understand a valid notation for composite state as infinite sum. The densities is the coefficient, the basis are are states, what is the differential?

Find smallest object that can have an invariant density (inner product). $\mathcal{c}= \int\limits_{T^*\mathbbm{Q}} \rho(q,k) \mathcal{e}(q,k)dq\wedge dk$

\begin{defn}\label{classical_phase_space}
The state space $\mathcal{S}$ for the particles of a homogeneous classical  system is a cotangent bundle $T^*\mathbbm{Q}$.
\end{defn}

\begin{justification}
	TODO
\end{justification}

TODO: generalization of inner product gives us $\int \rho_1(q, p) \rho_2(q, p) dq\wedge wp$. Space of continuous functions is not a complete metric space. Extend to square integrable. Get a Hilbert space. But topology is not a second countable: more than aleph 1. No physical justification, in fact we introduce unphysical functions (i.e. 0 rational, 1 irrational). Discontinuous functions not physical states: they are not distinguishable.

Density allows to count particle states. Example: two uniform distribution for the same amount of particles.

Area in each d.o.f. is number of possibilities. Total number of states is product of number of possibilities for independent d.o.f. Independent d.o.f. are orthogonal. Leads to omega metric. As in discrete case, conserved during evolution.

Also: we can search for omega looking from constraints. Must be linear. Must be skew-symmetric (applied to the same vector twice must be zero). Must be invariant by coord transformation. 

\begin{defn}\label{symplectic_manifold}
	The state space $\mathcal{S}$ for the particles of a homogeneous classical  system is a symplectic manifold with metric $\omega$, the canonical two-form.
\end{defn}

\begin{justification}
	TODO
\end{justification}

\subsection{Classical hamiltonian assumption}

% Det/rev for infinitesimal part
% Inconsistent: 1. X=AxB A'=F(A) B'=F(B) Isolated sub-systems. Does not work for measuring. 2. A'=F(A) B'=F(A,B) Ideal non-interfering measurement: deterministic but not reversible. 3. A'=F(A,B) B'=F(A,B) X det/rev whole, but A not deterministic itself. But if you consider X a sub-system of X and something else, you can make the same argument. Whole universe deterministic, but none of its parts. 4. A=As x Au ; B=Bs x Bu ; As' = F(As) Au=F(X) Bu=F(X) Bs = F(As, Au) ; more realistic picture. Still non-reversible (if As' = F(As), then nothing else can be F(As) and have bijective map).

We further characterize the relationship between the whole and the parts with the following assumption:

\begin{assump}[Classical hamiltonian assumption]\label{classical}
	The evolution of the classical homogeneous system under study is deterministic, reversible and reducible.
\end{assump}

In other words: if we know the state and the evolution for the whole system, we also know the evolution for all its parts; if we know the state and evolution for all the parts, we know the state and the evolution for the whole system. By smaller we do not mean spatial extent, but of smaller amount (e.g. less mass): even at the same spatial location we can imagine that there are infinitely many components (e.g. the mass, as a number, is continuously divisible(.

\begin{rationale}
	This is clearly a \emph{simplifying} assumption, and it is instructive to understand when it breaks down.
	
	The first problem is methodological. As we saw before, we need access to a deterministic and reversible process to be able to study a system and define a state. For two balls, we can imagine to isolate some pieces (small enough to be considered infinitesimal in respect to the ball but big enough to contain enough molecules to be considered homogenous) and then describe the collision between the two by describing what happens at each piece. The classical assumption holds. For an electron and a photon, we cannot take pieces of the electron or the photon, study them in isolation and then describe how each part moves during Compton scattering. The classical assumption does not hold: we do not have suitable physical processes at our disposal.
	
	The second problem is more conceptual. As we saw before, a system cannot be fully isolated from the environment. A certain amount of unstated part needs to remain so that the state of the system can be insulated from the external non-deterministic interaction. In practice, the state of each piece of our material is still given by the average of a collection of a large amount of molecules, their respective motion depending not just on the state of the system but on the external environment.  The classical assumption can therefore hold only if we assume that the description of each pieces is, again, not complete. Assuming it at a fundamental level would mean that the motion of each and every molecule of the system is only influenced by the state of the system, no external factor counts. As such, it would be a contradiction with it being a physical system: one we can study experimentally.
	
	While ultimately flawed, the classical assumption can be considered valid for a great number of macroscopic system, and that is why is very useful. One should be cautioned, though, that nowhere in the assumption the spatial extent of the system is mentioned. We may as well have a macroscopic system where a clear independent state cannot be assigned to each part, and the assumption would not hold.
\end{rationale}

We are now ready to capture the elements of our discussion and their properties through mathematical definitions.


\begin{defn}\label{canonical_transformation}
	A deterministic and reversible process is a diffeomorphism that preserves $\omega$, that is a canonical transformation.
\end{defn}

\begin{justification}
	TODO
\end{justification}

\begin{defn}\label{hamiltonian}
	A continuous deterministic and reversible process admits a potential $H$, and the laws of evolution are of the form (Hamilton equations).
\end{defn}

\begin{justification}
	TODO
\end{justification}



\section{Quantum systems}
%Note on composite systems and distinguishability. When putting together two classical system, just sum distributions. When putting together quantum system we have to clarify: combine to create one quantum system (sum in vector space) or two separate quantum system (symmetrized tensor product). In classical mechanics, 2a + 2b = (a+b) + (a+b). In quantum note |a> x |b> = |a>|b> + |b>|a> but (|a> + |b>) x (|a> + |b>) = |a>|a> + ... That is two quantum states in two different states is not the same as two quantum states spread equally over those two states. In classical they are.)

% Muon example. Consider a muon and its decay into an electron and two neutrinos: it is clear that the three outgoing particles have a state and trajectory of their own, it is clear that the resulting total mass and energy came from the muon. Yet, before the decay, we cannot ascribe an internal independent state and evolution to each part. The state of a muon is not some combination of the state of an electron and two neutrinos. That is: the unstated part is not just microstates.


%TODO: moved from classical
%This intuitive picture is unfortunately not suitable to be generalized to different contexts, so we develop another.

%Consider the following expression:
%\begin{align*}
%\mathcal{c} = \sum\limits_{i \in \mathbbm{I}} a_i %\mathcal{e}(i)
%\end{align*}
%$\mathbbm{I}$ represents all the possible values of a state variable representing the best description of an infinitesimal part that a composite system can give. $\mathcal{e}(i)$ represents a composite system made of a unit amount of particles, all prepared in the state determined by $i$. $a_i$ represents a transformation that changes the whole state, without affecting the value $i$ for any infinitesimal part. For a classical system, $i$ represents the full information about the infinitesimal part, and therefore includes the value of all state variables (i.e. the state vector); the only transformation left is increasing/decreasing the amount of particles by a scalar multiple. For a different system, where $i$ does not represent the full state of the parts, $\mathcal{e}(i)$ will be an ensemble and $a_i$ also represents an internal transformation of such ensemble. This is intuitively how we decouple the composite state into the part $i$ that describes the infinitesimal parts, and the part $a_i$ that describes how the parts are composed. Giving a full description of the state space of a decomposable system means fully characterizing these components: the state variable $\mathbbm{I}$ and the set of transformations $A$. In the classical case $\mathcal{e}(i)$ form a basis of a real vector space, where $i$ is the state vector for an infinitesimal part, and $a_i$ a real valued coefficient that represents the amount of particles in each state, which describes how the parts are combined.


\begin{thebibliography}{0}

\bibitem{Shannon} Shannon, C. E., ``A mathematical theory of communication'', The Bell System Technical Journal, Vol. 27, pp. 379--423, 623--656, (1948).
\bibitem{Jaynes} Jaynes, E. T., ``Information theory and statistical mechanics'', Statistical Physics 3, pp. 181--218, (1963).
\bibitem{classical_dynamics} J. V. Jos\'{e}, E. J. Saletan, ``Classical Dynamics'', Cambridge University Press, (1998).
\bibitem{Gromov} Gromov, M. L., ``Pseudo holomorphic curves in symplectic manifolds'', Inventiones Mathematicae 82, pp. 307--347, (1985).
\bibitem{deGosson} de Gosson, M. A., ``The symplectic camel and the uncertainty principle: the tip of an iceberg?'', Foundations of Physics 39, pp. 194--214, (2009).
\bibitem{Stewart} Stewart, I., ``The symplectic camel'', Nature 329, pp. 17--18, (1987).
\bibitem{Lanczos} Lanczos, C., ``The variational principles of mechanics'', University of Toronto Press, (1949).
\bibitem{Synge} Synge, J. L., ``Classical dynamics'', Encyclopedia of Physics Vol 3/1, Springer (1960).
\bibitem{Struckmeier} Struckmeier, J., ``Hamiltonian dynamics on the symplectic extended phase space for autonomous and non-autonomous systems'', J. Phys. A: Math. Gen. 38, pp. 1257--1278, (2005).

\end{thebibliography}

\end{document}
