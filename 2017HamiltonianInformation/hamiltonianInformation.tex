\newif\ifjournal
%\journaltrue
\journalfalse

\ifjournal
	\documentclass[smallextended]{svjour3} 
\else 
	\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}
\fi

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{dutchcal}
\usepackage{braket}
\usepackage{enumitem}

\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{calculator}
\usepackage{standalone}

\numberwithin{equation}{section}

\ifjournal
	\spnewtheorem{assump}{Assumption}{\bf}{\it}
	\renewcommand{\theassump}{\Roman{assump}}
	\spnewtheorem{prop}[equation]{Proposition}{\bf}{\rm}
	\spnewtheorem{thrm}[equation]{Theorem}{\bf}{\it}

	\newenvironment{rationale}{\emph{Rationale}.}{\hfill\(\qed\)}
	\newenvironment{justification}{\emph{Justification}.}{\hfill\(\qed\)}
	\renewenvironment{proof}{\emph{Proof}.}{\hfill\(\qed\)}
\else
	% Theorem definitions using amsthm

	\usepackage{amsthm}

	\renewcommand\thesubsection{\thesection.\Alph{subsection}}
	\renewcommand{\theequation}{\thesection.\arabic{equation}}

	\newtheorem{assump}{Assumption}
	\renewcommand*{\theassump}{\Roman{assump}}
	\newtheorem{thrm}[equation]{Theorem}

	\theoremstyle{definition}
	\newtheorem{prop}[equation]{Proposition}

	\newenvironment{rationale}{\emph{Rationale}.}{\qed}
	\newenvironment{justification}{\emph{Justification}.}{\qed}
	\renewenvironment{proof}{\emph{Proof}.}{\qed}

\fi


%TODO: decide on identity function typesetting
\newcommand{\id}{\textrm{id}}

\newcommand{\journal}[1]{\ifjournal#1\fi}
\newcommand{\arxiv}[1]{\ifjournal\else#1\fi}

\renewcommand{\descriptionlabel}[1]{%
	\hspace\labelsep \upshape\bfseries #1:%
}

\begin{document}

\title{DRAFT \\ Hamiltonian mechanics is conservation of information}
\author{Gabriele Carcassi, Christine A. Aidala}

\ifjournal
	\institute{G. Carcassi \at
		University of Michigan, Ann Arbor, MI 48109 \\
		\email{carcassi@umich.edu}           %  \\
	}
\else
	\affiliation{University of Michigan, Ann Arbor, MI 48109}
\fi

%\ifjournal
%	% As the page margins are so large, the full title does not fit
%	\titlerunning{From physical principles to classical and quantum particle mechanics}
%\fi

\date{\today}

% Journal format has this before the abstract
\journal{\maketitle}
	
\begin{abstract}
\textbf{This manuscript is a work in progress.} Ideas are constantly reshaped to find more precise and elegant arguments. It is provided as is to stimulate discussion.  \textbf{Make sure you have the latest version from http://assumptionsofphysics.org}

In this work we show that the canonical transformations of classical Hamiltonian mechanics are exactly the transformations that preserve information.
\end{abstract}

\arxiv{\maketitle}

\section{Introduction}

\section{Distributions as observer dependent}

The main concept at the heart of this work is the idea of a distribution or density. What we want to do is talk about its information entropy (i.e. how many bits are required to identify one of its elements) and its evolution in time. But before we can do that, we have to clarify some issues that appear when dealing with distributions: mainly that they are not invariant under change of variables. Partially to address this problem, differential geometry and probability theory formalize the concept in different ways. This creates further difficulties as we want to mix ideas from both camps. For example, we will want to talk about marginal distributions on manifolds. In this section we want to clarify what the problems are, how they are formally addressed in both disciplines so that we can build a bridge and highlight the connection.

We start with our physical intuition. Distributions are widely used tools in all branches of physics: mass or charge densities, probability distributions over energy, cross sections and so on. All these objects are the limit of a ration between two quantities which can be integrated to get final values. For example, if $\mathbf{x}$ is the coordinate of a point, $V$ is a volume and $\rho$ is a mass distribution, the integral
\begin{equation}
m =\int_V \rho(\mathbf{x}) dV
\end{equation}
tells us the total mass within the given volume.

Similarly, we can define the information entropy for the distribution as
\begin{equation}
I(\rho) =-\int \rho(\mathbf{x}) \ln (\rho(\mathbf{x})) dV
\end{equation}
which quantifies the amount of information needed to identify an element within the distribution.

Densities, though, are not invariant. Intuitively, if we change units of the denominator the distribution changes value. For example, $1$ kg/m becomes $1000$ kg/km: the number changes but the units allow us to understand that the overall quantity remains the same. In general, if $\mathbf{y}=\mathbf{y}(\mathbf{x})$ we can recover how the density changes by requiring that the mass in each volume is the same no matter what coordinates are used. That is
\begin{align*}
m &=\int_V \rho_x(\mathbf{x}) dV_x = \int_V \rho_y(\mathbf{y}) dV_y \\
&=\int_V\rho_y(\mathbf{y}) \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right| dV_x
\end{align*}
\begin{equation}
\rho_x(\mathbf{x}) = \rho_y(\mathbf{y}) \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right|
\end{equation}
the density is multiplied by the Jacobian of the transformation.

Similarly, we can see that information entropy is not invariant
\begin{align*}
I(\rho_x) &=-\int \rho_x(\mathbf{x}) \ln (\rho_x(\mathbf{x})) dV_x \\
&=-\int \rho_y(\mathbf{y}) \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right| \ln (\rho_y(\mathbf{y}) \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right|) dV_x \\
&=-\int \rho_y(\mathbf{y}) \ln (\rho_y(\mathbf{y}) \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right|) dV_y \\
&=-\int \rho_y(\mathbf{y}) \ln (\rho_y(\mathbf{y})) dV_y - \int \rho_y(\mathbf{y}) \ln (\left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right|) dV_y \\&=I(\rho_y) - \int \rho_y(\mathbf{y}) \ln (\left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right|) dV_y
\end{align*}
The reason is that, in the continuous case, the information entropy is not an absolute number of bits, but it's relative to uniform distribution over a unit size. That is: if the distribution is over a continuous variable, it would take an infinite amount of information to specify an element. But we can tell, for example, that to specify an element within a uniform distribution between $[0,2]$ we need a bit more than to specify an element within a uniform distribution between $[0,1]$. If we change coordinate what happens is that we change our reference therefore the information entropy appears to change.

The punchline is the following: while physically we think as a distribution as a number associated to a point, mathematically it is not. That is, given the manifold $\mathcal{M}$ on which we define the distribution, the latter is \emph{not} $\rho : \mathcal{M} \rightarrow \mathbb{R}$. That would be invariant under coordinate transformation, as a point is a point in any coordinates.

In differential geometry, a density is defined at each point as a function that takes a parallelepiped, identified by n vectors, and returns a number. That is $\mu : V \times ... \times V \rightarrow \mathbb{R}$. Physically, this includes both the distribution and the volume: it bundles $\rho(\mathbf{x}) dV$ because this is the invariant object. While this allows us to recast integration in a coordinate independent way, the object we have does not quite map to our physics intuition. It also does not allow to easily express quantities, like information entropy or expectations, that operate on $\rho(\mathbf{x})$ as a function at a point.

In statistics, instead, the approach is different. The probability measure is a function that takes a set of events and assigns it a number between zero and one. That is $Pr : \sigma(\Omega) \rightarrow [0,1]$ does not on how the events are labeled. Random variables $X : \Omega \rightarrow \mathbb{R}$ are simply real valued functions of the events, not coordinates for some point on a manifold. For distributions, the starting point is the cumulative distribution function $F_X(x)=Pr(X<x)$ and the probability density function is its Radonâ€“Nikodym derivative $\rho_X = \frac{dF_X}{dx}$. That is: the integral is the primal objects while the density is derived.\footnote{This actually makes more physical sense: what we measure is the amount of mass in a finite region and the density is the limit for a smaller and smaller one.} We therefore have densities defined at values, but they are not invariant objects. This is not an issue as probabilities are not objective entities. For example, there is no absolute probability for an electron to be in a state of spin up: it depends on the ensemble considered (e.g. an electron gas, electron pairs in a superconductor, ...). Therefore it does not matter if they depend on the choice of labels.

Given the situation, though, a question arises spontaneously: what if we wanted to define probabilities at each point on a manifold stuch that they were invariant? What kind of mathematical structure would we get? We already know that such manifold would have to be differentiable, because only on a set of coordinates that are differentiable we can define the Jacobian and therefore we can properly transform the density and the information entropy. What other structure is needed?

As we'll see, the answer is: we need, and only need, a symplectic structure. We need the type of structure we have in phase space for Hamiltonian mechanics. And if we require such distributions and their information entropy to be preserved over time, we get Hamiltonian motion.

\section{Something}

\begin{prop}
	Transported coordinates are canonical. Distributions are function of the point and are transported.
\end{prop}

$\mathcal{M} = T^*\mathcal{Q}$. $(T^*\mathcal{Q}, \omega)$, $(q^i, p_j)$ set of canonical coordinates, $F: T^*\mathcal{Q} \rightarrow T^*\mathcal{Q}$ such that $\omega = F^* \omega$ where $F^*$ is the pullback. $\hat{q}^i = q^i \circ F^{-1}$ and $\hat{p}_i = p_i \circ F^{-1}$.

$F^* (\sum d\hat{q}^i \wedge d\hat{p}_i) =\sum F^* (d\hat{q}^i \wedge d\hat{p}_i) =  \sum (F^* d\hat{q}^i)\wedge (F^* d\hat{p}_i) = \sum d(\hat{q}^i \circ F)\wedge d(\hat{p}_i \circ F) = \sum d(q^i \circ F^{-1} \circ F)\wedge d(p_i \circ F^{-1} \circ F) = \sum dq^i\wedge dp_i = \omega = F^* \omega$. $\omega =  \sum d\hat{q}^i \wedge d\hat{p}_i$. $(\hat{q}^i, \hat{p}_j)$ is a set of canonical coordinates.

$\rho(q^i, p_j) : \mathbb{R}^{2n} \rightarrow \mathbb{R}$ integrable and differentiable. $\rho (q^i, p_j) \, dq^n \wedge dp_n = \rho (q^i, p_j) \, \frac{\omega^n}{n!} = \rho (q^i(\hat{q}^i, \hat{p}_j), p_j(\hat{q}^i, \hat{p}_j)) \, d\hat{q}^n \wedge d\hat{p}_n$. $\rho$, defined on the symplectic structure, transforms like a function. We can write $\rho : (T^*\mathcal{Q}, \omega) \rightarrow \mathbb{R}$.

$\hat{\rho} : (T^*\mathcal{Q}, \omega) \rightarrow \mathbb{R}$ such that $\hat{\rho} = \rho \circ F^{-1 }$. $\hat{\rho}(\hat{q}^i, \hat{p}_j) = \hat{\rho} \circ (\hat{q}^i, \hat{p}_j)^{-1} = \rho \circ F^{-1} \circ (q^i \circ F^{-1}, p_i \circ F^{-1}) ^ {-1}= \rho \circ F^{-1} \circ F \circ (q^i, p_j) ^ {-1} = \rho \circ (q^i, p_j) ^ {-1} = \rho (q^i, p_j)$.

\begin{prop}
	Canonical transformations transport marginal distributions and preserve independence.
\end{prop}

$\mathcal{Q} = \mathcal{Q}_1 \times \mathcal{Q}_2$. $(T^*\mathcal{Q}, \omega)= (T^*\mathcal{Q}_1, \omega_1) \times (T^*\mathcal{Q}_2, \omega_2)$, $(q^i, p_j)$ set of canonical coordinates divides into $(q_1^i, p_{1j})$ and $(q_2^i, p_{2j})$ canonical coordinates.

$\rho : (T^*\mathcal{Q}, \omega) \rightarrow \mathbb{R}$. $\frac{(\omega)^n}{n!} = \frac{(\omega_1 + \omega_2)^n}{n!} = {n \choose n_1} \frac{(\omega_1)^{n_1} \wedge (\omega_2)^{n_2}}{n!} = \frac{n!}{n_1!n_2!}\frac{(\omega_1)^{n_1} \wedge (\omega_2)^{n_2}}{n!} = \frac{(\omega_1)^{n_1}}{n_1!} \wedge \frac{(\omega_2)^{n_2}}{n_2!} $. $\int_{T^*\mathcal{Q_1}}\rho \frac{(\omega)^n}{n!} = \int_{T^*\mathcal{Q_1}}\rho \frac{(\omega_1)^{n_1}}{n_1!} \wedge \frac{(\omega_2)^{n_2}}{n_2!} = \frac{(\omega_2)^{n_2}}{n_2!} \wedge \int_{T^*\mathcal{Q_1}}\rho \frac{(\omega_1)^{n_1}}{n_1!}$. Let $\rho_2 : (T^*\mathcal{Q}_2, \omega) \rightarrow \mathbb{R}$ such that $\rho_2 = \int_{T^*\mathcal{Q_1}}\rho \frac{(\omega_1)^{n_1}}{n_1!}$. $\int_{T^*\mathcal{Q_1}}\rho \frac{(\omega)^n}{n!} = \rho_2 \frac{(\omega_2)^{n_2}}{n_2!}$. Similarly, $\int_{T^*\mathcal{Q_2}}\rho \frac{(\omega)^n}{n!} = \rho_1 \frac{(\omega_1)^{n_1}}{n_1!}$ with $\rho_1 : (T^*\mathcal{Q}_1, \omega) \rightarrow \mathbb{R}$.

$F(T^*\mathcal{Q}, \omega) = F((T^*\mathcal{Q}_1, \omega_1) \times (T^*\mathcal{Q}_2, \omega_2)) = F_1(T^*\mathcal{Q}_1, \omega_1) \times F_2(T^*\mathcal{Q}_2, \omega_2) = (\hat{\mathcal{M}}_1, \hat{\omega}_1) \times (\hat{\mathcal{M}}_2, \hat{\omega}_2)$.
Let $\hat{\rho}_1 : (\hat{\mathcal{M}}_1, \hat{\omega}_1) \rightarrow \mathbb{R}$ and $\hat{\rho}_2 : (\hat{\mathcal{M}}_2, \hat{\omega}_2) \rightarrow \mathbb{R}$ such that $\hat{\rho}_1 = \rho_1 \circ F^{-1 }$ and $\hat{\rho}_2 = \rho_2 \circ F^{-1 }$.  $\hat{\rho}_1(\hat{q}_1^i, \hat{p}_{1j})=\rho_1(q_1^i, p_{1j})$ and $\hat{\rho}_2(\hat{q}_2^i, \hat{p}_{2j})=\rho_2(q_2^i, p_{2j})$.

Let $\rho=\rho_1\rho_2$. $\hat{\rho}(\hat{q}^i, \hat{p}_j) =\rho(q^i, p_j)=\rho_1(q_1^i, p_{1j})\rho_2(q_2^i, p_{2j})=\hat{\rho}_1(\hat{q}_1^i, \hat{p}_{1j})\hat{\rho}_2(\hat{q}_2^i, \hat{p}_{2j})$. $\hat{\rho}=\hat{\rho}_1\hat{\rho}_2$.

Prove the opposite: from transport and independence of marginal distributions to canonical transformation.

Suppose we can write $\rho=\prod\rho_i(q^i,p_i)$. Then $1 = \int_{\mathcal{M}} \rho dq^n\wedge dp_n$ but also $1 = \prod \int_{\mathcal{M}_i} \rho_i dq^i\wedge dp_i$. $\int_{\mathcal{M}} \prod\rho_i(q^i,p_i) dq^n\wedge dp_n = \prod \int_{\mathcal{M}_i} \rho_i dq^i\wedge dp_i$. $dq^n\wedge dp_n = \bigwedge dq^i\wedge dp_i = \frac{(\sum dq^i\wedge dp_i)^n}{n!}$. $\omega = \sum dq^i\wedge dp_i$. $(q^i,p_j)$ are canonical coordinates. Suppose $F$ is such that $\hat{\rho}(\hat{q}^i, \hat{p}_j) = \rho(q^i, p_j)$, $\hat{\rho}_1(\hat{q}_1^i, \hat{p}_{1j})=\rho_1(q_1^i, p_{1j})$ and $\hat{\rho}_2(\hat{q}_2^i, \hat{p}_{2j})=\rho_2(q_2^i, p_{2j})$. For each pair $(q^i, p_i)$ we have $\int_{\mathcal{M}_i} \rho_i dq^i \wedge dp_i = \int_{\hat{\mathcal{M}}_i} \hat{\rho}_i d\hat{q}^i \wedge d\hat{p}_i$ for all $\rho$ and $\hat{\rho}$: $dq^i \wedge dp_i = d\hat{q}^i \wedge d\hat{p}_i$. $\sum dq^i \wedge dp_i = \sum d\hat{q}^i \wedge d\hat{p}_i$. $(\hat{q}^i,\hat{p}_j)$ are canonical coordinates. $F$ is a canonical transformation.

Conjecture: the manifold for the marginal distributions is not always a cotangent bundle.


\begin{prop}
	Canonical transformations transport mutual information.
\end{prop}

If $F$ canonical transformation, $I_{ij} = \int_{T^*\mathcal{Q}} \rho \log (\frac{\rho}{\rho_i \rho_j}) dq^n dp_n = \int_{T^*\mathcal{Q}} \hat{\rho} \log (\frac{\hat{\rho}}{\hat{\rho}_i \hat{\rho}_j}) dq^n dp_n = \hat{I}_{ij}$.

Suppose $I_{ij} = \int_{T^*\mathcal{Q}} \rho \log (\frac{\rho}{\rho_i \rho_j}) dq^n dp_n = \int_{T^*\mathcal{Q}} \hat{\rho} \log (\frac{\hat{\rho}}{\hat{\rho}_i \hat{\rho}_j}) dq^n dp_n = \hat{I}_{ij}$ for all $\rho$. Then $\rho(q^i, p_j)=\hat{\rho}(\hat{q}^i, \hat{p}_j)$ and $\rho_i(q^i, p_i)=\hat{\rho}_i(\hat{q}^i, \hat{p}_i)$. In particular, if $I_{ij} = \hat{I}_{ij} = 0$ if $i\neq j$ we can write $\rho=\prod\rho_i(q^i,p_i) = \hat{\rho} = \prod\rho_i(\hat{q}^i,\hat{p}_i)$. $F$ is a canonical transformation as it preserves independence of marginal distributions.

\begin{thebibliography}{0}
	
\end{thebibliography}

\end{document}
