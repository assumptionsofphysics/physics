\newif\ifjournal
%\journaltrue
\journalfalse

\ifjournal
	\documentclass[smallextended]{svjour3} 
\else 
	\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}
\fi

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{dutchcal}
\usepackage{braket}
\usepackage{enumitem}

\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{calculator}
\usepackage{standalone}

\numberwithin{equation}{section}

\ifjournal
	\spnewtheorem{assump}{Assumption}{\bf}{\it}
	\renewcommand{\theassump}{\Roman{assump}}
	\spnewtheorem{prop}[equation]{Proposition}{\bf}{\rm}
	\spnewtheorem{thrm}[equation]{Theorem}{\bf}{\it}

	\newenvironment{rationale}{\emph{Rationale}.}{\hfill\(\qed\)}
	\newenvironment{justification}{\emph{Justification}.}{\hfill\(\qed\)}
	\renewenvironment{proof}{\emph{Proof}.}{\hfill\(\qed\)}
\else
	% Theorem definitions using amsthm

	\usepackage{amsthm}

	\renewcommand\thesubsection{\thesection.\Alph{subsection}}
	\renewcommand{\theequation}{\thesection.\arabic{equation}}

	\newtheorem{assump}{Assumption}
	\renewcommand*{\theassump}{\Roman{assump}}
	\newtheorem{thrm}[equation]{Theorem}

	\theoremstyle{definition}
	\newtheorem{prop}[equation]{Proposition}

	\newenvironment{rationale}{\emph{Rationale}.}{\qed}
	\newenvironment{justification}{\emph{Justification}.}{\qed}
	\renewenvironment{proof}{\emph{Proof}.}{\qed}

\fi


%TODO: decide on identity function typesetting
\newcommand{\id}{\textrm{id}}

\newcommand{\journal}[1]{\ifjournal#1\fi}
\newcommand{\arxiv}[1]{\ifjournal\else#1\fi}

\renewcommand{\descriptionlabel}[1]{%
	\hspace\labelsep \upshape\bfseries #1:%
}

\begin{document}

\title{DRAFT \\ Hamiltonian mechanics is conservation of information}
\author{Gabriele Carcassi, Christine A. Aidala}

\ifjournal
	\institute{G. Carcassi \at
		University of Michigan, Ann Arbor, MI 48109 \\
		\email{carcassi@umich.edu}           %  \\
	}
\else
	\affiliation{University of Michigan, Ann Arbor, MI 48109}
\fi

%\ifjournal
%	% As the page margins are so large, the full title does not fit
%	\titlerunning{From physical principles to classical and quantum particle mechanics}
%\fi

\date{\today}

% Journal format has this before the abstract
\journal{\maketitle}
	
\begin{abstract}
\textbf{This manuscript is a work in progress.} Ideas are constantly reshaped to find more precise and elegant arguments. It is provided as is to stimulate discussion.  \textbf{Make sure you have the latest version from http://assumptionsofphysics.org}

In this work we show that the canonical transformations of classical Hamiltonian mechanics are exactly the transformations that preserve information.
\end{abstract}

\arxiv{\maketitle}

\section{Introduction}

\section{Distributions as observer dependent}

The main subjects of this work are distributions and densities: their properties, like marginal distributions and information entropy, and their time evolution. Let's briefly review their basic features.

Distributions may be used to represent different  physical entities: mass or charge densities, probability distributions over energy, cross sections and so on. All these objects are the limit of a ratio between two quantities, which can be integrated over an arbitrary region to get a finite value. For example, if $\mathbf{x}$ is the coordinate of a point, $V$ is a volume, $M$ is the total mass and $\rho$ is the normalized mass distribution, the integral
\begin{equation}
m_V =\int_V M \rho(\mathbf{x}) dV
\end{equation}
tells us the total mass within the given volume.

Similarly, we can define the information entropy for the distribution as
\begin{equation}
I(\rho) =-\int \rho(\mathbf{x}) \ln (\rho(\mathbf{x})) dV
\end{equation}
which quantifies the amount of information needed to identify an element within the distribution. Technically, given the more convenient choice of the natural logarithm, the unit of information is the nat, which is equal to $\frac{1}{\ln 2}$ bits. However, in the discussion we will refer to it as measured in bits, as we believe that the majority of the readers is more conformable with the latter unit.

Densities, though, are not invariant. Intuitively, if we change units of the denominator the distribution changes value. For example, $1$ kg/m becomes $1000$ kg/km: the number changes but the units allow us to understand that the overall quantity remains the same. In general, if $\mathbf{y}=\mathbf{y}(\mathbf{x})$ we can recover how the density changes by requiring that the mass in each volume is the same no matter what coordinates are used. That is
\begin{align*}
m &=\int_V \rho_x(\mathbf{x}) dV_x = \int_V \rho_y(\mathbf{y}) dV_y \\
&=\int_V\rho_y(\mathbf{y}) \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right| dV_x
\end{align*}
\begin{equation}\label{density_transformation}
\rho_x(\mathbf{x}) = \rho_y(\mathbf{y}) \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right|
\end{equation}
the density is multiplied by the Jacobian of the transformation.

Similarly, we can also see that information entropy is not invariant
\begin{align*}
I(\rho_x) &=-\int \rho_x(\mathbf{x}) \ln (\rho_x(\mathbf{x})) dV_x \\
&=-\int \rho_y(\mathbf{y}) \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right| \ln (\rho_y(\mathbf{y}) \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right|) dV_x \\
&=-\int \rho_y(\mathbf{y}) \ln (\rho_y(\mathbf{y}) \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right|) dV_y \\
&=-\int \rho_y(\mathbf{y}) \ln (\rho_y(\mathbf{y})) dV_y - \int \rho_y(\mathbf{y}) \ln (\left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right|) dV_y
\end{align*}
\begin{equation}\label{entropy_transformation}
I(\rho_x) =I(\rho_y) - \int \rho_y(\mathbf{y}) \ln (\left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right|) dV_y
\end{equation}
The reason is that, in the continuous case, the information entropy is not an absolute number of bits. In fact, if the distribution is over a continuous variable, it would take an infinite amount of information to specify an element. Instead it's the relative number of bits needed to identify an element compared to a uniform distribution over a unit size. For example, to specify an element within a uniform distribution between $[0,2]$ we need a bit more than to specify an element within a uniform distribution between $[0,1]$. If we change coordinates we also change our reference therefore the information entropy appears to change.

The punchline is the following: while physically we may regard a distribution as a number associated to a point, mathematically it is not. Given the manifold $\mathcal{M}$ on which we define the distribution, the latter is \emph{not} $\rho : \mathcal{M} \rightarrow \mathbb{R}$. That would be invariant under coordinate transformation, as a point is a point in any coordinates. As a consequence, differential geometry and statistics have a different way to handle them. 

In differential geometry, a density is defined at each point as a function that takes a parallelepiped, identified by n vectors, and returns a number. That is $\mu : V \times ... \times V \rightarrow \mathbb{R}$. Physically, this includes both the distribution and the volume: it bundles $\rho(\mathbf{x}) dV$ because this is the invariant object. While this allows us to recast integration in a coordinate independent way, the object we have does not quite map to our physics intuition. It also does not allow to easily express quantities, like information entropy or expectations, that operate on $\rho(\mathbf{x})$ as a function at a point.

In statistics, one starts with the probability measure: a function $Pr : \sigma(\Omega) \rightarrow [0,1]$ that takes a set of events and assigns it a number between zero and one. This function is independent on how the events are labeled. Random variables $X : \Omega \rightarrow \mathbb{R}$ are simply real valued functions of the events (i.e. they are not thought as coordinates for some point on a manifold). For each random variable we can define its cumulative distribution function $F_X(x)=Pr(X<x)$ and the probability density function is its Radonâ€“Nikodym derivative $\rho_X = \frac{dF_X}{dx}$. That is: the integral is the primal object while the density is derived.\footnote{This actually makes more physical sense: what we measure is the amount of mass in a finite region and the density is the limit for a smaller and smaller one.} We therefore have densities defined at values, but they are not invariant objects. However, as probabilities depend on our choice of ensemble, it is not inconsistent that they also depend on our labeling. For example, there is no absolute probability for an electron to be in a state of spin up: it depends on the ensemble considered (e.g. an electron gas, electron pairs in a superconductor, ...).

Given the situation, though, a question arises spontaneously: what if we wanted to define densities at each point on a manifold such that they \emph{were} invariant? This may be desirable for something like a mass distribution, which we would like to think like coordinate independent function of the point: the fact that some mass is in a particular state should be objective and therefore independent on the choice of coordinates. What kind of mathematical structure would we get? We already know that such manifold would have to be differentiable, because only on a set of coordinates that are differentiable we can define the Jacobian and therefore we can properly transform the density and the information entropy. What other structure is needed?

As we'll see, the answer is: we need, and only need, a symplectic structure: the type of structure we have in phase space for Hamiltonian mechanics, with its conjugate quantities and Poisson brackets. And if we require such distributions and their information entropy to be preserved over time, we get Hamiltonian motion.

\section{Single degree of freedom}

We begin with studying the simplest case, one with a single degree of freedom. In this case, it is easy to show that canonical transformations preserve both the probability and the information entropy.

Suppose we have a distribution in phase space $\rho(q, p)$ and perform a canonical change variables $Q=Q(q.p)$ $P=P(q,p)$. The Jacobian of the transformation $|J| = \frac{\partial Q}{\partial q}\frac{\partial P}{\partial p} - \frac{\partial P}{\partial q} \frac{\partial Q}{\partial p} = \{Q, P\}$ is equal to the Poisson bracket between $Q$ and $P$. But the definition of a canonical transformation is that it preserves the Poisson brackets: $\{q,p\} = \{Q, P\} = 1$. Therefore we have
\begin{align}
\rho(q, p) &= \rho(Q,P) \\
I(\rho(q,p)) &= I(\rho(Q,P))
\end{align}
The density and the information entropy associated with it do not change.

Conversely, suppose we have a transformation that preserves the density and the information entropy. Then the Jacobian determinant of such transformation has to be unitary everywhere. But, for one degree of freedom, the Jacobian coincides with the Poisson bracket: the transformation must be canonical.

We recast this reasoning using symplectic geometry, as this will be useful later. For each distribution $\rho(q, p)$ we can associate a two form $\rho(q, p) \omega$ where $\omega=dq\wedge dp$ is the two form that returns the are of an infinitesimal surface. In other words, the distribution is the component of the two form associated with it. The integral of the two form over a region will give us the integral of the distribution over a region.

Under a change of coordinates, we have $\rho(Q, P) dQ \wedge dP = \rho(q,p) dq \wedge dp$. If the transformation is canonical $\omega = dq \wedge dp = dQ \wedge dP$. Therefore $\rho(Q, P) \omega = \rho(q,p) \omega$ which gives $\rho(Q, P) = \rho(q,p)$. Conversely, if $\rho(Q, P) = \rho(q,p)$, we find $dQ \wedge dP = dq \wedge dp = \omega$: the transformation is canonical. In other words: a symplectic structure is exactly the structure needed to define coordinate invariant distributions on a 2 dimensional smooth manifold.

If we require the distribution and its information entropy to be preserved through time evolution, then time evolution is a canonical transformation as well. As such, it allows a potential $H : \mathcal{M} \rightarrow \mathbb{R}$ such that
\begin{align}
S^q \equiv \frac{dq}{dt} &= \frac{\partial H}{\partial p} \\
S^p \equiv \frac{dp}{dt} &= - \frac{\partial H}{\partial q}
\end{align} 
where the vector $S$ tells us the infinitesimal displacement of each state. As time passes, both the distribution value and the volume where it is defined are conserved. This means that $S$, as a vector field, is divergence free. As such, it admits a potential such that $S = \mathrm{curl}(H)$ which leads to the previous equations.

We are now in a position to ``have our invariant distribution and integrate it too". On the symplectic structure, our density is a scalar function $\rho : \mathcal{M} \rightarrow \mathbb{R}$. It has the two form $\rho \omega$ associate with it, in case we want to speak the language of differential geometry, and it has the function $\rho(q,p)$ in local coordinate, in case we want to speak the language of statistics and define quantities like the information entropy.

\section{Multiple degree of freedom}

The generalization to multiple degrees of freedom is conceptually very simple: not only we will need to preserve the distribution and the entropy, but we will do it for each independent degree of freedom. One needs to be careful, though, as it may be misconstrued to mean something much stronger that it actually is.

$\varrho = \{q,p\} \rho(q,p)$

$\omega = \frac{dq\wedge dp}{\{q,p\}}$

$\varrho \omega = \rho(q,p) dq\wedge dp$


\section{Something}

\begin{prop}
	Transported coordinates are canonical. Distributions are function of the point and are transported.
\end{prop}

$\mathcal{M} = T^*\mathcal{Q}$. $(T^*\mathcal{Q}, \omega)$, $(q^i, p_j)$ set of canonical coordinates, $F: T^*\mathcal{Q} \rightarrow T^*\mathcal{Q}$ such that $\omega = F^* \omega$ where $F^*$ is the pullback. $\hat{q}^i = q^i \circ F^{-1}$ and $\hat{p}_i = p_i \circ F^{-1}$.

$F^* (\sum d\hat{q}^i \wedge d\hat{p}_i) =\sum F^* (d\hat{q}^i \wedge d\hat{p}_i) =  \sum (F^* d\hat{q}^i)\wedge (F^* d\hat{p}_i) = \sum d(\hat{q}^i \circ F)\wedge d(\hat{p}_i \circ F) = \sum d(q^i \circ F^{-1} \circ F)\wedge d(p_i \circ F^{-1} \circ F) = \sum dq^i\wedge dp_i = \omega = F^* \omega$. $\omega =  \sum d\hat{q}^i \wedge d\hat{p}_i$. $(\hat{q}^i, \hat{p}_j)$ is a set of canonical coordinates.

$\rho(q^i, p_j) : \mathbb{R}^{2n} \rightarrow \mathbb{R}$ integrable and differentiable. $\rho (q^i, p_j) \, dq^n \wedge dp_n = \rho (q^i, p_j) \, \frac{\omega^n}{n!} = \rho (q^i(\hat{q}^i, \hat{p}_j), p_j(\hat{q}^i, \hat{p}_j)) \, d\hat{q}^n \wedge d\hat{p}_n$. $\rho$, defined on the symplectic structure, transforms like a function. We can write $\rho : (T^*\mathcal{Q}, \omega) \rightarrow \mathbb{R}$.

$\hat{\rho} : (T^*\mathcal{Q}, \omega) \rightarrow \mathbb{R}$ such that $\hat{\rho} = \rho \circ F^{-1 }$. $\hat{\rho}(\hat{q}^i, \hat{p}_j) = \hat{\rho} \circ (\hat{q}^i, \hat{p}_j)^{-1} = \rho \circ F^{-1} \circ (q^i \circ F^{-1}, p_i \circ F^{-1}) ^ {-1}= \rho \circ F^{-1} \circ F \circ (q^i, p_j) ^ {-1} = \rho \circ (q^i, p_j) ^ {-1} = \rho (q^i, p_j)$.

\begin{prop}
	Canonical transformations transport marginal distributions and preserve independence.
\end{prop}

$\mathcal{Q} = \mathcal{Q}_1 \times \mathcal{Q}_2$. $(T^*\mathcal{Q}, \omega)= (T^*\mathcal{Q}_1, \omega_1) \times (T^*\mathcal{Q}_2, \omega_2)$, $(q^i, p_j)$ set of canonical coordinates divides into $(q_1^i, p_{1j})$ and $(q_2^i, p_{2j})$ canonical coordinates.

$\rho : (T^*\mathcal{Q}, \omega) \rightarrow \mathbb{R}$. $\frac{(\omega)^n}{n!} = \frac{(\omega_1 + \omega_2)^n}{n!} = {n \choose n_1} \frac{(\omega_1)^{n_1} \wedge (\omega_2)^{n_2}}{n!} = \frac{n!}{n_1!n_2!}\frac{(\omega_1)^{n_1} \wedge (\omega_2)^{n_2}}{n!} = \frac{(\omega_1)^{n_1}}{n_1!} \wedge \frac{(\omega_2)^{n_2}}{n_2!} $. $\int_{T^*\mathcal{Q_1}}\rho \frac{(\omega)^n}{n!} = \int_{T^*\mathcal{Q_1}}\rho \frac{(\omega_1)^{n_1}}{n_1!} \wedge \frac{(\omega_2)^{n_2}}{n_2!} = \frac{(\omega_2)^{n_2}}{n_2!} \wedge \int_{T^*\mathcal{Q_1}}\rho \frac{(\omega_1)^{n_1}}{n_1!}$. Let $\rho_2 : (T^*\mathcal{Q}_2, \omega) \rightarrow \mathbb{R}$ such that $\rho_2 = \int_{T^*\mathcal{Q_1}}\rho \frac{(\omega_1)^{n_1}}{n_1!}$. $\int_{T^*\mathcal{Q_1}}\rho \frac{(\omega)^n}{n!} = \rho_2 \frac{(\omega_2)^{n_2}}{n_2!}$. Similarly, $\int_{T^*\mathcal{Q_2}}\rho \frac{(\omega)^n}{n!} = \rho_1 \frac{(\omega_1)^{n_1}}{n_1!}$ with $\rho_1 : (T^*\mathcal{Q}_1, \omega) \rightarrow \mathbb{R}$.

$F(T^*\mathcal{Q}, \omega) = F((T^*\mathcal{Q}_1, \omega_1) \times (T^*\mathcal{Q}_2, \omega_2)) = F_1(T^*\mathcal{Q}_1, \omega_1) \times F_2(T^*\mathcal{Q}_2, \omega_2) = (\hat{\mathcal{M}}_1, \hat{\omega}_1) \times (\hat{\mathcal{M}}_2, \hat{\omega}_2)$.
Let $\hat{\rho}_1 : (\hat{\mathcal{M}}_1, \hat{\omega}_1) \rightarrow \mathbb{R}$ and $\hat{\rho}_2 : (\hat{\mathcal{M}}_2, \hat{\omega}_2) \rightarrow \mathbb{R}$ such that $\hat{\rho}_1 = \rho_1 \circ F^{-1 }$ and $\hat{\rho}_2 = \rho_2 \circ F^{-1 }$.  $\hat{\rho}_1(\hat{q}_1^i, \hat{p}_{1j})=\rho_1(q_1^i, p_{1j})$ and $\hat{\rho}_2(\hat{q}_2^i, \hat{p}_{2j})=\rho_2(q_2^i, p_{2j})$.

Let $\rho=\rho_1\rho_2$. $\hat{\rho}(\hat{q}^i, \hat{p}_j) =\rho(q^i, p_j)=\rho_1(q_1^i, p_{1j})\rho_2(q_2^i, p_{2j})=\hat{\rho}_1(\hat{q}_1^i, \hat{p}_{1j})\hat{\rho}_2(\hat{q}_2^i, \hat{p}_{2j})$. $\hat{\rho}=\hat{\rho}_1\hat{\rho}_2$.

Prove the opposite: from transport and independence of marginal distributions to canonical transformation.

Suppose we can write $\rho=\prod\rho_i(q^i,p_i)$. Then $1 = \int_{\mathcal{M}} \rho dq^n\wedge dp_n$ but also $1 = \prod \int_{\mathcal{M}_i} \rho_i dq^i\wedge dp_i$. $\int_{\mathcal{M}} \prod\rho_i(q^i,p_i) dq^n\wedge dp_n = \prod \int_{\mathcal{M}_i} \rho_i dq^i\wedge dp_i$. $dq^n\wedge dp_n = \bigwedge dq^i\wedge dp_i = \frac{(\sum dq^i\wedge dp_i)^n}{n!}$. $\omega = \sum dq^i\wedge dp_i$. $(q^i,p_j)$ are canonical coordinates. Suppose $F$ is such that $\hat{\rho}(\hat{q}^i, \hat{p}_j) = \rho(q^i, p_j)$, $\hat{\rho}_1(\hat{q}_1^i, \hat{p}_{1j})=\rho_1(q_1^i, p_{1j})$ and $\hat{\rho}_2(\hat{q}_2^i, \hat{p}_{2j})=\rho_2(q_2^i, p_{2j})$. For each pair $(q^i, p_i)$ we have $\int_{\mathcal{M}_i} \rho_i dq^i \wedge dp_i = \int_{\hat{\mathcal{M}}_i} \hat{\rho}_i d\hat{q}^i \wedge d\hat{p}_i$ for all $\rho$ and $\hat{\rho}$: $dq^i \wedge dp_i = d\hat{q}^i \wedge d\hat{p}_i$. $\sum dq^i \wedge dp_i = \sum d\hat{q}^i \wedge d\hat{p}_i$. $(\hat{q}^i,\hat{p}_j)$ are canonical coordinates. $F$ is a canonical transformation.

Conjecture: the manifold for the marginal distributions is not always a cotangent bundle.


\begin{prop}
	Canonical transformations transport mutual information.
\end{prop}

If $F$ canonical transformation, $I_{ij} = \int_{T^*\mathcal{Q}} \rho \log (\frac{\rho}{\rho_i \rho_j}) dq^n dp_n = \int_{T^*\mathcal{Q}} \hat{\rho} \log (\frac{\hat{\rho}}{\hat{\rho}_i \hat{\rho}_j}) dq^n dp_n = \hat{I}_{ij}$.

Suppose $I_{ij} = \int_{T^*\mathcal{Q}} \rho \log (\frac{\rho}{\rho_i \rho_j}) dq^n dp_n = \int_{T^*\mathcal{Q}} \hat{\rho} \log (\frac{\hat{\rho}}{\hat{\rho}_i \hat{\rho}_j}) dq^n dp_n = \hat{I}_{ij}$ for all $\rho$. Then $\rho(q^i, p_j)=\hat{\rho}(\hat{q}^i, \hat{p}_j)$ and $\rho_i(q^i, p_i)=\hat{\rho}_i(\hat{q}^i, \hat{p}_i)$. In particular, if $I_{ij} = \hat{I}_{ij} = 0$ if $i\neq j$ we can write $\rho=\prod\rho_i(q^i,p_i) = \hat{\rho} = \prod\rho_i(\hat{q}^i,\hat{p}_i)$. $F$ is a canonical transformation as it preserves independence of marginal distributions.

\begin{thebibliography}{0}
	
\end{thebibliography}

\end{document}
